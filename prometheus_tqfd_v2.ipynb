{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd25 PROMETHEUS-TQFD v2.0\n",
        "### Dual-AI Tabula Rasa Chess Training System\n",
        "\n",
        "Dieses Notebook trainiert zwei verschiedene Schach-KIs:\n",
        "1. **ATLAS**: AlphaZero-Stil (ResNet + MCTS)\n",
        "2. **ENTROPY v2.0**: Physik-inspirierter Hybrid-Ansatz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83d\udd27 1. Setup & Installation\n",
        "!pip install --quiet python-chess numpy torch psutil lz4 safetensors plotly streamlit pyngrok\n",
        "\n",
        "import multiprocessing as mp\n",
        "try:\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "    print(\"\u2705 Multiprocessing set to 'spawn'\")\n",
        "except RuntimeError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \ud83d\udcc1 2. Module Generation\n",
        "import os\n",
        "os.makedirs('prometheus_tqfd', exist_ok=True)\n",
        "os.makedirs('prometheus_tqfd/atlas', exist_ok=True)\n",
        "os.makedirs('prometheus_tqfd/dashboard', exist_ok=True)\n",
        "os.makedirs('prometheus_tqfd/entropy', exist_ok=True)\n",
        "os.makedirs('prometheus_tqfd/evaluation', exist_ok=True)\n",
        "os.makedirs('prometheus_tqfd/orchestration', exist_ok=True)\n",
        "os.makedirs('prometheus_tqfd/utils', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/__init__.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/config.py\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import os\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "@dataclass\n",
        "class PrometheusConfig:\n",
        "    # === SYSTEM ===\n",
        "    run_id: str = field(default_factory=lambda: datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
        "    base_dir: Path = Path('/content/prometheus_runs')\n",
        "    use_drive: bool = True\n",
        "    seed: int = 42\n",
        "    \n",
        "    # === ATLAS (AlphaZero-Stil) ===\n",
        "    atlas_input_channels: int = 19\n",
        "    atlas_res_blocks: int = 8              # Reduziert f\u00fcr Colab\n",
        "    atlas_channels: int = 256\n",
        "    atlas_learning_rate: float = 1e-3\n",
        "    atlas_weight_decay: float = 1e-4\n",
        "    atlas_batch_size: int = 128            # Hardware-adaptiv\n",
        "    atlas_replay_size: int = 300_000\n",
        "    atlas_mcts_simulations: int = 100\n",
        "    atlas_mcts_cpuct: float = 2.5\n",
        "    atlas_dirichlet_alpha: float = 0.3\n",
        "    atlas_dirichlet_epsilon: float = 0.25\n",
        "    atlas_temperature_moves: int = 30\n",
        "    atlas_temperature_init: float = 1.0\n",
        "    atlas_temperature_final: float = 0.1\n",
        "    \n",
        "    # === ENTROPY v2.0 (Hybrid Physik) ===\n",
        "    entropy_input_channels: int = 22       # 19 Board + 3 Felder\n",
        "    entropy_res_blocks: int = 6\n",
        "    entropy_channels: int = 128\n",
        "    entropy_learning_rate: float = 3e-4\n",
        "    entropy_batch_size: int = 64\n",
        "    entropy_replay_size: int = 200_000\n",
        "    \n",
        "    # Mini-Rollout Parameter\n",
        "    entropy_rollout_depth: int = 3\n",
        "    entropy_rollout_count: int = 5\n",
        "    entropy_temperature_start: float = 3.0\n",
        "    entropy_temperature_end: float = 0.3\n",
        "    entropy_temperature_decay: float = 0.9999\n",
        "    \n",
        "    # Loss-Gewichtung (muss 1.0 ergeben)\n",
        "    entropy_loss_outcome: float = 0.30     # Spielergebnis\n",
        "    entropy_loss_mobility: float = 0.25    # Eigene Optionen\n",
        "    entropy_loss_pressure: float = 0.20    # Druck auf Gegner\n",
        "    entropy_loss_stability: float = 0.15   # TD auf Energie\n",
        "    entropy_loss_novelty: float = 0.10     # RND Exploration\n",
        "    \n",
        "    # Physik-Konstanten\n",
        "    physics_energy_king: float = 1000.0\n",
        "    physics_energy_queen: float = 9.5\n",
        "    physics_energy_rook: float = 5.25\n",
        "    physics_energy_bishop: float = 3.33\n",
        "    physics_energy_knight: float = 3.05\n",
        "    physics_energy_pawn: float = 1.0\n",
        "    physics_diffusion_sigma: float = 2.5\n",
        "    physics_field_alpha: float = 1.0\n",
        "    physics_field_beta: float = 0.5\n",
        "    \n",
        "    # === TACTICS DETECTOR ===\n",
        "    tactics_boost_strength: float = 1.0      # Volle St\u00e4rke\n",
        "    tactics_boost_decay: float = 1.0         # Kein Decay (permanent)\n",
        "    tactics_mate_boost: float = 50.0         # Boost f\u00fcr Matt-in-1\n",
        "    tactics_hanging_boost: float = 5.0       # Boost f\u00fcr Figurenrettung\n",
        "    tactics_threat_boost: float = 3.0        # Boost f\u00fcr Verteidigung\n",
        "\n",
        "    # === TRAINING ===\n",
        "    min_buffer_before_training: int = 5_000\n",
        "    weight_publish_interval: int = 50\n",
        "    gpu_priority_atlas: float = 0.6        # 60% Priorit\u00e4t f\u00fcr ATLAS\n",
        "    num_atlas_selfplay_workers: int = 1\n",
        "    num_entropy_selfplay_workers: int = 1\n",
        "    \n",
        "    # === CHECKPOINTS ===\n",
        "    checkpoint_micro_interval: int = 5     # Minuten\n",
        "    checkpoint_light_interval: int = 15    # Minuten\n",
        "    checkpoint_full_interval: int = 60     # Minuten\n",
        "    checkpoint_keep_n: int = 3\n",
        "    \n",
        "    # === EVALUATION ===\n",
        "    eval_interval_games: int = 5_000\n",
        "    eval_games_atlas_entropy: int = 50\n",
        "    eval_games_vs_random: int = 20\n",
        "    eval_games_vs_heuristic: int = 20\n",
        "    elo_initial: float = 1000.0\n",
        "    elo_k_factor: float = 32.0\n",
        "    \n",
        "    # === DASHBOARD ===\n",
        "    dashboard_port: int = 8501\n",
        "    dashboard_refresh_seconds: float = 2.0\n",
        "    \n",
        "    # === RESILIENZ ===\n",
        "    heartbeat_timeout: float = 60.0\n",
        "    max_process_restarts: int = 3\n",
        "    oom_batch_reduction: float = 0.5\n",
        "    oom_min_batch_size: int = 8\n",
        "\n",
        "@dataclass\n",
        "class HardwareConfig:\n",
        "    device: str\n",
        "    gpu_name: Optional[str]\n",
        "    vram_gb: Optional[float]\n",
        "    ram_gb: float\n",
        "    cpu_cores: int\n",
        "    is_colab: bool\n",
        "\n",
        "def adjust_config_for_hardware(config: PrometheusConfig, hw: HardwareConfig) -> PrometheusConfig:\n",
        "    \"\"\"\n",
        "    Erkennt Hardware und setzt optimale Parameter.\n",
        "    \"\"\"\n",
        "    if hw.device == 'cpu':\n",
        "        config.atlas_batch_size = 32\n",
        "        config.entropy_batch_size = 16\n",
        "        config.atlas_res_blocks = 5\n",
        "        config.atlas_mcts_simulations = 50\n",
        "        config.atlas_replay_size = 100_000\n",
        "        return config\n",
        "\n",
        "    vram_gb = hw.vram_gb or 0.0\n",
        "\n",
        "    if vram_gb >= 40:  # A100\n",
        "        config.atlas_batch_size = 512\n",
        "        config.atlas_mcts_simulations = 400\n",
        "        config.atlas_res_blocks = 12\n",
        "        config.atlas_replay_size = 1_000_000\n",
        "        config.entropy_batch_size = 256\n",
        "        config.entropy_replay_size = 500_000\n",
        "    elif vram_gb >= 16:  # V100 / T4-Pro\n",
        "        # Defaults sind bereits T4-optimiert\n",
        "        config.atlas_batch_size = 128\n",
        "        config.atlas_mcts_simulations = 100\n",
        "        config.atlas_res_blocks = 8\n",
        "        config.atlas_replay_size = 300_000\n",
        "    elif vram_gb >= 12:  # T4-Free\n",
        "        config.atlas_batch_size = 128\n",
        "        config.atlas_mcts_simulations = 100\n",
        "        config.atlas_res_blocks = 8\n",
        "        config.atlas_replay_size = 300_000\n",
        "    else:  # < 8GB (z.B. K80 oder kleine lokale GPU)\n",
        "        config.atlas_batch_size = 32\n",
        "        config.atlas_mcts_simulations = 50\n",
        "        config.atlas_res_blocks = 5\n",
        "        config.atlas_replay_size = 100_000\n",
        "        config.entropy_batch_size = 32\n",
        "        config.entropy_replay_size = 100_000\n",
        "\n",
        "    # Worker-Anzahl anpassen\n",
        "    if hw.cpu_cores >= 8:\n",
        "        config.num_atlas_selfplay_workers = 2\n",
        "        config.num_entropy_selfplay_workers = 2\n",
        "    else:\n",
        "        config.num_atlas_selfplay_workers = 1\n",
        "        config.num_entropy_selfplay_workers = 1\n",
        "\n",
        "    return config\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/encoding.py\n",
        "import chess\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class BoardEncoder:\n",
        "    def __init__(self, use_history=False, history_len=8):\n",
        "        self.use_history = use_history\n",
        "        self.history_len = history_len\n",
        "        self.num_channels = 19 + (12 * (history_len - 1)) if use_history else 19\n",
        "\n",
        "    def encode(self, board: chess.Board) -> torch.Tensor:\n",
        "        # [C, 8, 8]\n",
        "        tensor = np.zeros((19, 8, 8), dtype=np.float32)\n",
        "\n",
        "        # Piece channels\n",
        "        for square, piece in board.piece_map().items():\n",
        "            rank, file = divmod(square, 8)\n",
        "            # White: 0-5, Black: 6-11\n",
        "            channel = piece.piece_type - 1 + (0 if piece.color == chess.WHITE else 6)\n",
        "            tensor[channel, rank, file] = 1.0\n",
        "\n",
        "        # Castling rights\n",
        "        if board.has_kingside_castling_rights(chess.WHITE): tensor[12, :, :] = 1.0\n",
        "        if board.has_queenside_castling_rights(chess.WHITE): tensor[13, :, :] = 1.0\n",
        "        if board.has_kingside_castling_rights(chess.BLACK): tensor[14, :, :] = 1.0\n",
        "        if board.has_queenside_castling_rights(chess.BLACK): tensor[15, :, :] = 1.0\n",
        "\n",
        "        # En passant\n",
        "        if board.ep_square is not None:\n",
        "            rank, file = divmod(board.ep_square, 8)\n",
        "            tensor[16, rank, file] = 1.0\n",
        "\n",
        "        # Halfmove clock\n",
        "        tensor[17, :, :] = board.halfmove_clock / 100.0\n",
        "\n",
        "        # Side to move\n",
        "        if board.turn == chess.WHITE:\n",
        "            tensor[18, :, :] = 1.0\n",
        "\n",
        "        return torch.from_numpy(tensor)\n",
        "\n",
        "class MoveEncoder:\n",
        "    def __init__(self):\n",
        "        self.move_to_idx = {}\n",
        "        self.idx_to_move = {}\n",
        "        self._create_mapping()\n",
        "\n",
        "    def _create_mapping(self):\n",
        "        idx = 0\n",
        "        # directions: (dr, df)\n",
        "        # Queen moves\n",
        "        directions = [\n",
        "            (1, 0), (1, 1), (0, 1), (-1, 1),\n",
        "            (-1, 0), (-1, -1), (0, -1), (1, -1)\n",
        "        ]\n",
        "\n",
        "        for from_sq in range(64):\n",
        "            from_rank, from_file = divmod(from_sq, 8)\n",
        "\n",
        "            # Queen-like moves\n",
        "            for d_idx, (dr, df) in enumerate(directions):\n",
        "                for dist in range(1, 8):\n",
        "                    to_rank, to_file = from_rank + dr * dist, from_file + df * dist\n",
        "                    if 0 <= to_rank < 8 and 0 <= to_file < 8:\n",
        "                        # This move exists. We need a unique ID for (from_sq, d_idx, dist)\n",
        "                        # Actually AlphaZero maps each move to 73 planes of 8x8\n",
        "                        # 0-55: Queen moves (8 dir * 7 dist)\n",
        "                        # 56-63: Knight moves\n",
        "                        # 64-72: Underpromotions\n",
        "                        pass\n",
        "\n",
        "        # Let's use a simpler mapping that's consistent with the spec: 64 * 73\n",
        "        # plane_idx:\n",
        "        # 0..55: Queen moves (direction * 7 + (distance - 1))\n",
        "        # 56..63: Knight moves\n",
        "        # 64..72: Underpromotions\n",
        "\n",
        "        # We don't really need to pre-populate everything if we can compute it\n",
        "        pass\n",
        "\n",
        "    def get_plane_and_sq(self, move: chess.Move):\n",
        "        from_sq = move.from_square\n",
        "        to_sq = move.to_square\n",
        "        from_rank, from_file = divmod(from_sq, 8)\n",
        "        to_rank, to_file = divmod(to_sq, 8)\n",
        "        dr, df = to_rank - from_rank, to_file - from_file\n",
        "\n",
        "        # Underpromotions\n",
        "        if move.promotion and move.promotion != chess.QUEEN:\n",
        "            # 64-72: 3 directions (df: -1, 0, 1) x 3 pieces (N, B, R)\n",
        "            # Pieces: N=2, B=3, R=4\n",
        "            piece_idx = move.promotion - 2 # 0, 1, 2\n",
        "            dir_idx = df + 1 # 0, 1, 2\n",
        "            plane = 64 + piece_idx * 3 + dir_idx\n",
        "            return plane, from_sq\n",
        "\n",
        "        # Knight moves\n",
        "        knight_moves = [\n",
        "            (2, 1), (1, 2), (-1, 2), (-2, 1),\n",
        "            (-2, -1), (-1, -2), (1, -2), (2, -1)\n",
        "        ]\n",
        "        if (dr, df) in knight_moves:\n",
        "            plane = 56 + knight_moves.index((dr, df))\n",
        "            return plane, from_sq\n",
        "\n",
        "        # Queen moves\n",
        "        directions = [\n",
        "            (1, 0), (1, 1), (0, 1), (-1, 1),\n",
        "            (-1, 0), (-1, -1), (0, -1), (1, -1)\n",
        "        ]\n",
        "        abs_dr, abs_df = abs(dr), abs(df)\n",
        "        dist = max(abs_dr, abs_df)\n",
        "        if (dr // dist, df // dist) in directions and (abs_dr == 0 or abs_df == 0 or abs_dr == abs_df):\n",
        "            d_idx = directions.index((dr // dist, df // dist))\n",
        "            plane = d_idx * 7 + (dist - 1)\n",
        "            return plane, from_sq\n",
        "\n",
        "        raise ValueError(f\"Invalid move for encoding: {move}\")\n",
        "\n",
        "    def move_to_index(self, move: chess.Move) -> int:\n",
        "        plane, from_sq = self.get_plane_and_sq(move)\n",
        "        return from_sq * 73 + plane\n",
        "\n",
        "    def index_to_move(self, index: int, board: chess.Board) -> chess.Move:\n",
        "        from_sq, plane = divmod(index, 73)\n",
        "        from_rank, from_file = divmod(from_sq, 8)\n",
        "\n",
        "        if plane < 56:\n",
        "            # Queen move\n",
        "            d_idx, dist_m1 = divmod(plane, 7)\n",
        "            dist = dist_m1 + 1\n",
        "            directions = [(1, 0), (1, 1), (0, 1), (-1, 1), (-1, 0), (-1, -1), (0, -1), (1, -1)]\n",
        "            dr, df = directions[d_idx]\n",
        "            to_rank, to_file = from_rank + dr * dist, from_file + df * dist\n",
        "            to_sq = to_rank * 8 + to_file\n",
        "            move = chess.Move(from_sq, to_sq)\n",
        "            # Check for promotion to Queen (default)\n",
        "            if board.piece_at(from_sq) and board.piece_at(from_sq).piece_type == chess.PAWN:\n",
        "                if (to_rank == 7 and board.turn == chess.WHITE) or (to_rank == 0 and board.turn == chess.BLACK):\n",
        "                    move.promotion = chess.QUEEN\n",
        "            return move\n",
        "        elif plane < 64:\n",
        "            # Knight move\n",
        "            knight_moves = [(2, 1), (1, 2), (-1, 2), (-2, 1), (-2, -1), (-1, -2), (1, -2), (2, -1)]\n",
        "            dr, df = knight_moves[plane - 56]\n",
        "            to_rank, to_file = from_rank + dr, from_file + df\n",
        "            to_sq = to_rank * 8 + to_file\n",
        "            return chess.Move(from_sq, to_sq)\n",
        "        else:\n",
        "            # Underpromotion\n",
        "            piece_idx, dir_idx = divmod(plane - 64, 3)\n",
        "            df = dir_idx - 1\n",
        "            piece = piece_idx + 2\n",
        "            to_rank = 7 if board.turn == chess.WHITE else 0\n",
        "            to_file = from_file + df\n",
        "            to_sq = to_rank * 8 + to_file\n",
        "            return chess.Move(from_sq, to_sq, promotion=piece)\n",
        "\n",
        "    def get_legal_mask(self, board: chess.Board) -> torch.Tensor:\n",
        "        mask = torch.zeros(4672, dtype=torch.bool)\n",
        "        for move in board.legal_moves:\n",
        "            mask[self.move_to_index(move)] = True\n",
        "        return mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/physics.py\n",
        "import chess\n",
        "import numpy as np\n",
        "import torch\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "\n",
        "class PhysicsFieldCalculator:\n",
        "    \"\"\"\n",
        "    Berechnet physik-inspirierte Felder aus Board-Zustand.\n",
        "    \n",
        "    Ausgabe: Tensor[3, 8, 8]\n",
        "    - Kanal 0: Masse-Feld M(x,y) - Gau\u00df-gewichtete Figurenenergie\n",
        "    - Kanal 1: Mobilit\u00e4ts-Feld F(x,y) - Angriffs-/Bewegungsdruck\n",
        "    - Kanal 2: Druck-Feld P(x,y) - Kombination aus M und F\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig):\n",
        "        self.energies = {\n",
        "            chess.KING: config.physics_energy_king,\n",
        "            chess.QUEEN: config.physics_energy_queen,\n",
        "            chess.ROOK: config.physics_energy_rook,\n",
        "            chess.BISHOP: config.physics_energy_bishop,\n",
        "            chess.KNIGHT: config.physics_energy_knight,\n",
        "            chess.PAWN: config.physics_energy_pawn,\n",
        "        }\n",
        "        self.sigma = config.physics_diffusion_sigma\n",
        "        self.alpha = config.physics_field_alpha\n",
        "        self.beta = config.physics_field_beta\n",
        "        \n",
        "        self._precompute_gaussian_kernel()\n",
        "    \n",
        "    def _precompute_gaussian_kernel(self):\n",
        "        # 15x15 kernel um jede Position abzudecken\n",
        "        size = 15\n",
        "        center = size // 2\n",
        "        kernel = np.zeros((size, size))\n",
        "        for i in range(size):\n",
        "            for j in range(size):\n",
        "                dist_sq = (i - center)**2 + (j - center)**2\n",
        "                kernel[i, j] = np.exp(-dist_sq / (2 * self.sigma**2))\n",
        "        self.kernel = kernel\n",
        "\n",
        "    def _gaussian_at(self, x0, y0):\n",
        "        # Returns a 8x8 grid with a gaussian centered at x0, y0\n",
        "        grid = np.zeros((8, 8))\n",
        "        size = 15\n",
        "        center = size // 2\n",
        "        \n",
        "        for r in range(8):\n",
        "            for c in range(8):\n",
        "                dr, dc = r - y0, c - x0\n",
        "                if abs(dr) <= center and abs(dc) <= center:\n",
        "                    grid[r, c] = self.kernel[center + dr, center + dc]\n",
        "        return grid\n",
        "\n",
        "    def compute(self, board: chess.Board) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Berechnet alle drei Felder.\n",
        "        \"\"\"\n",
        "        mass = self._compute_mass_field(board)\n",
        "        mobility = self._compute_mobility_field(board)\n",
        "        \n",
        "        # Druck-Feld: alpha*M + beta*F (gegl\u00e4ttet)\n",
        "        # Hier vereinfacht als direkte Kombination\n",
        "        pressure = self.alpha * mass + self.beta * mobility\n",
        "        \n",
        "        # Stack and to tensor\n",
        "        fields = np.stack([mass, mobility, pressure])\n",
        "        return torch.from_numpy(fields).float()\n",
        "    \n",
        "    def _compute_mass_field(self, board: chess.Board) -> np.ndarray:\n",
        "        field = np.zeros((8, 8))\n",
        "        for square in chess.SQUARES:\n",
        "            piece = board.piece_at(square)\n",
        "            if piece:\n",
        "                energy = self.energies[piece.piece_type]\n",
        "                sign = 1 if piece.color == chess.WHITE else -1\n",
        "                x0, y0 = square % 8, square // 8\n",
        "                field += sign * energy * self._gaussian_at(x0, y0)\n",
        "        \n",
        "        # Normalisierung\n",
        "        if np.max(np.abs(field)) > 0:\n",
        "            field = field / np.max(np.abs(field))\n",
        "        return field\n",
        "    \n",
        "    def _compute_mobility_field(self, board: chess.Board) -> np.ndarray:\n",
        "        field = np.zeros((8, 8))\n",
        "        for color in [chess.WHITE, chess.BLACK]:\n",
        "            sign = 1 if color == chess.WHITE else -1\n",
        "            for square in chess.SQUARES:\n",
        "                attackers = len(board.attackers(color, square))\n",
        "                x, y = square % 8, square // 8\n",
        "                field[y, x] += sign * attackers\n",
        "        \n",
        "        # Normalisieren auf [-1, 1]\n",
        "        if field.max() - field.min() > 0:\n",
        "            field = 2 * (field - field.min()) / (field.max() - field.min()) - 1\n",
        "        return field\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/tactics.py\n",
        "import chess\n",
        "import torch\n",
        "from typing import Dict, Any, List, Optional, Tuple\n",
        "from prometheus_tqfd.encoding import MoveEncoder\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "\n",
        "class TacticsDetector:\n",
        "    \"\"\"\n",
        "    Regelbasierter Detektor f\u00fcr kritische taktische Muster.\n",
        "    KEINE ML - reine Schachlogik via python-chess.\n",
        "    \n",
        "    Erkennt:\n",
        "    - Matt in 1 (f\u00fcr uns)\n",
        "    - Matt-Drohung (vom Gegner)\n",
        "    - H\u00e4ngende Figuren\n",
        "    - Verf\u00fcgbare Schachs\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig):\n",
        "        self.config = config\n",
        "        self.move_encoder = MoveEncoder()\n",
        "        self.boost_strength = config.tactics_boost_strength\n",
        "        self.decay_rate = config.tactics_boost_decay\n",
        "        self.current_strength = self.boost_strength\n",
        "\n",
        "    def decay_step(self):\n",
        "        self.current_strength *= self.decay_rate\n",
        "\n",
        "    def detect(self, board: chess.Board) -> Dict[str, Any]:\n",
        "        threats = {\n",
        "            'mate_in_1': None,          # Der Matt-Zug, wenn vorhanden\n",
        "            'mate_threat': False,        # Gegner droht Matt\n",
        "            'hanging_pieces': [],        # Ungesch\u00fctzte eigene Figuren (square, piece)\n",
        "            'checks_available': 0,       # Anzahl m\u00f6glicher Schachs\n",
        "            'captures_available': [],    # Schlagz\u00fcge\n",
        "        }\n",
        "        \n",
        "        # Matt in 1 suchen\n",
        "        for move in board.legal_moves:\n",
        "            board.push(move)\n",
        "            if board.is_checkmate():\n",
        "                threats['mate_in_1'] = move\n",
        "                board.pop()\n",
        "                break\n",
        "            board.pop()\n",
        "        \n",
        "        # Gegner-Matt-Drohung pr\u00fcfen\n",
        "        # Wir machen einen Nullzug um zu sehen ob der Gegner Matt setzen kann\n",
        "        if not board.is_check():\n",
        "            board.push(chess.Move.null())\n",
        "            for move in board.legal_moves:\n",
        "                board.push(move)\n",
        "                if board.is_checkmate():\n",
        "                    threats['mate_threat'] = True\n",
        "                    board.pop()\n",
        "                    break\n",
        "                board.pop()\n",
        "            board.pop()\n",
        "        \n",
        "        # H\u00e4ngende Figuren finden\n",
        "        for square in chess.SQUARES:\n",
        "            piece = board.piece_at(square)\n",
        "            if piece and piece.color == board.turn:\n",
        "                if board.is_attacked_by(not board.turn, square):\n",
        "                    defenders = len(list(board.attackers(board.turn, square)))\n",
        "                    attackers = len(list(board.attackers(not board.turn, square)))\n",
        "                    if defenders < attackers:\n",
        "                        threats['hanging_pieces'].append((square, piece))\n",
        "        \n",
        "        # Schachs z\u00e4hlen und Schlagz\u00fcge sammeln\n",
        "        for move in board.legal_moves:\n",
        "            if board.gives_check(move):\n",
        "                threats['checks_available'] += 1\n",
        "            if board.is_capture(move):\n",
        "                threats['captures_available'].append(move)\n",
        "        \n",
        "        return threats\n",
        "    \n",
        "    def get_tactical_boost(self, board: chess.Board) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Erzeugt Boost-Tensor f\u00fcr taktisch kritische Z\u00fcge.\n",
        "        Wird zur Policy addiert (vor Softmax).\n",
        "        \"\"\"\n",
        "        boost = torch.zeros(4672)\n",
        "        threats = self.detect(board)\n",
        "        \n",
        "        # Matt in 1: Massiver Boost\n",
        "        if threats['mate_in_1']:\n",
        "            idx = self.move_encoder.move_to_index(threats['mate_in_1'])\n",
        "            boost[idx] = self.config.tactics_mate_boost\n",
        "        \n",
        "        # Matt-Drohung: Defensive Z\u00fcge boosten\n",
        "        if threats['mate_threat']:\n",
        "            for move in board.legal_moves:\n",
        "                # Schach geben ist oft defensiv\n",
        "                if board.gives_check(move):\n",
        "                    idx = self.move_encoder.move_to_index(move)\n",
        "                    boost[idx] += self.config.tactics_threat_boost\n",
        "                # K\u00f6nig bewegen\n",
        "                piece = board.piece_at(move.from_square)\n",
        "                if piece and piece.piece_type == chess.KING:\n",
        "                    idx = self.move_encoder.move_to_index(move)\n",
        "                    boost[idx] += self.config.tactics_threat_boost * 0.5\n",
        "        \n",
        "        # H\u00e4ngende Figuren retten\n",
        "        for square, piece in threats['hanging_pieces']:\n",
        "            for move in board.legal_moves:\n",
        "                if move.from_square == square:\n",
        "                    idx = self.move_encoder.move_to_index(move)\n",
        "                    value = self._piece_value(piece.piece_type)\n",
        "                    boost[idx] += value * (self.config.tactics_hanging_boost / 10.0)\n",
        "        \n",
        "        return boost * self.current_strength\n",
        "    \n",
        "    def _piece_value(self, piece_type: int) -> float:\n",
        "        values = {\n",
        "            chess.PAWN: 1, chess.KNIGHT: 3, chess.BISHOP: 3,\n",
        "            chess.ROOK: 5, chess.QUEEN: 9, chess.KING: 100\n",
        "        }\n",
        "        return values.get(piece_type, 0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/atlas/__init__.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/atlas/network.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(channels)\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(channels)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        residual = x\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.bn2(self.conv2(x))\n",
        "        x = F.relu(x + residual)\n",
        "        return x\n",
        "\n",
        "class AtlasNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard AlphaZero-Architektur:\n",
        "    - Input: [batch, 19, 8, 8]\n",
        "    - Residual Tower: N Bl\u00f6cke mit Skip-Connections\n",
        "    - Policy Head: Wahrscheinlichkeiten \u00fcber 4672 Z\u00fcge\n",
        "    - Value Head: Gewinnwahrscheinlichkeit [-1, +1]\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig):\n",
        "        super().__init__()\n",
        "        C = config.atlas_channels\n",
        "        \n",
        "        # Input Block\n",
        "        self.input_block = nn.Sequential(\n",
        "            nn.Conv2d(19, C, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(C),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Residual Tower\n",
        "        self.res_blocks = nn.ModuleList([\n",
        "            ResidualBlock(C) \n",
        "            for _ in range(config.atlas_res_blocks)\n",
        "        ])\n",
        "        \n",
        "        # Policy Head\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Conv2d(C, 32, kernel_size=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 64, 4672)\n",
        "        )\n",
        "        \n",
        "        # Value Head\n",
        "        self.value_head = nn.Sequential(\n",
        "            nn.Conv2d(C, 4, kernel_size=1),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(4 * 64, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        x = self.input_block(x)\n",
        "        for block in self.res_blocks:\n",
        "            x = block(x)\n",
        "        policy_logits = self.policy_head(x)\n",
        "        value = self.value_head(x)\n",
        "        return policy_logits, value\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/atlas/mcts.py\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import chess\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, Optional, List, Tuple\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.atlas.network import AtlasNetwork\n",
        "from prometheus_tqfd.encoding import BoardEncoder, MoveEncoder\n",
        "\n",
        "@dataclass\n",
        "class MCTSNode:\n",
        "    state: chess.Board\n",
        "    parent: Optional['MCTSNode'] = None\n",
        "    parent_action: Optional[chess.Move] = None\n",
        "    children: Dict[chess.Move, 'MCTSNode'] = field(default_factory=dict)\n",
        "    visit_count: int = 0\n",
        "    total_value: float = 0.0\n",
        "    prior: float = 0.0\n",
        "    \n",
        "    @property\n",
        "    def q_value(self) -> float:\n",
        "        if self.visit_count == 0:\n",
        "            return 0.0\n",
        "        return self.total_value / self.visit_count\n",
        "\n",
        "class MCTS:\n",
        "    \"\"\"\n",
        "    Monte Carlo Tree Search mit PUCT.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig, network: AtlasNetwork, device: str):\n",
        "        self.config = config\n",
        "        self.network = network\n",
        "        self.device = device\n",
        "        self.encoder = BoardEncoder()\n",
        "        self.move_encoder = MoveEncoder()\n",
        "    \n",
        "    def search(self, root_board: chess.Board, num_simulations: int = None) -> MCTSNode:\n",
        "        if num_simulations is None:\n",
        "            num_simulations = self.config.atlas_mcts_simulations\n",
        "        \n",
        "        root = MCTSNode(state=root_board.copy())\n",
        "        \n",
        "        # Initial expansion of root\n",
        "        self._evaluate_and_expand(root)\n",
        "        self._add_dirichlet_noise(root)\n",
        "        \n",
        "        for _ in range(num_simulations):\n",
        "            node = self._select(root)\n",
        "            value = self._evaluate_and_expand(node)\n",
        "            self._backpropagate(node, value)\n",
        "        \n",
        "        return root\n",
        "    \n",
        "    def _ucb_score(self, parent: MCTSNode, child: MCTSNode) -> float:\n",
        "        \"\"\"PUCT-Formel\"\"\"\n",
        "        c = self.config.atlas_mcts_cpuct\n",
        "        q = child.q_value\n",
        "        u = c * child.prior * math.sqrt(parent.visit_count) / (1 + child.visit_count)\n",
        "        return q + u\n",
        "    \n",
        "    def _select(self, node: MCTSNode) -> MCTSNode:\n",
        "        \"\"\"Traversiere bis Blatt mit max UCB\"\"\"\n",
        "        while node.children and not node.state.is_game_over():\n",
        "            node = max(node.children.values(), key=lambda c: self._ucb_score(node, c))\n",
        "        return node\n",
        "    \n",
        "    def _evaluate_and_expand(self, node: MCTSNode) -> float:\n",
        "        \"\"\"Netzwerk-Inference und Expansion\"\"\"\n",
        "        if node.state.is_game_over():\n",
        "            result = node.state.result()\n",
        "            if result == \"1-0\":\n",
        "                v = 1.0 if node.state.turn == chess.BLACK else -1.0\n",
        "            elif result == \"0-1\":\n",
        "                v = -1.0 if node.state.turn == chess.BLACK else 1.0\n",
        "            else:\n",
        "                v = 0.0\n",
        "            return v\n",
        "        \n",
        "        # Netzwerk-Inference\n",
        "        tensor = self.encoder.encode(node.state).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            policy_logits, value = self.network(tensor)\n",
        "        \n",
        "        # Legal-Mask und Softmax\n",
        "        legal_mask = self.move_encoder.get_legal_mask(node.state).to(self.device)\n",
        "        policy_logits[~legal_mask.unsqueeze(0)] = float('-inf')\n",
        "        policy = F.softmax(policy_logits, dim=1).squeeze(0)\n",
        "        \n",
        "        # Kinder erstellen\n",
        "        for move in node.state.legal_moves:\n",
        "            idx = self.move_encoder.move_to_index(move)\n",
        "            new_board = node.state.copy()\n",
        "            new_board.push(move)\n",
        "            child = MCTSNode(\n",
        "                state=new_board,\n",
        "                parent=node,\n",
        "                parent_action=move,\n",
        "                prior=policy[idx].item()\n",
        "            )\n",
        "            node.children[move] = child\n",
        "        \n",
        "        return value.item()\n",
        "    \n",
        "    def _backpropagate(self, node: MCTSNode, value: float):\n",
        "        \"\"\"Value entlang Pfad propagieren mit Vorzeichenwechsel\"\"\"\n",
        "        while node is not None:\n",
        "            node.visit_count += 1\n",
        "            node.total_value += value\n",
        "            value = -value  # Perspektivwechsel\n",
        "            node = node.parent\n",
        "    \n",
        "    def _add_dirichlet_noise(self, root: MCTSNode):\n",
        "        \"\"\"Exploration-Noise am Wurzelknoten\"\"\"\n",
        "        if not root.children:\n",
        "            return\n",
        "        alpha = self.config.atlas_dirichlet_alpha\n",
        "        epsilon = self.config.atlas_dirichlet_epsilon\n",
        "        noise = np.random.dirichlet([alpha] * len(root.children))\n",
        "        for i, child in enumerate(root.children.values()):\n",
        "            child.prior = (1 - epsilon) * child.prior + epsilon * noise[i]\n",
        "    \n",
        "    def get_policy_target(self, root: MCTSNode, temperature: float) -> torch.Tensor:\n",
        "        \"\"\"Normalisierte Visit-Counts als Policy-Target\"\"\"\n",
        "        policy = torch.zeros(4672)\n",
        "        visits = []\n",
        "        indices = []\n",
        "        \n",
        "        for move, child in root.children.items():\n",
        "            idx = self.move_encoder.move_to_index(move)\n",
        "            visits.append(child.visit_count)\n",
        "            indices.append(idx)\n",
        "        \n",
        "        visits = torch.tensor(visits, dtype=torch.float32)\n",
        "        \n",
        "        if temperature <= 0.01:\n",
        "            # Greedy\n",
        "            best_idx = visits.argmax()\n",
        "            policy[indices[best_idx]] = 1.0\n",
        "        else:\n",
        "            # Temperature-Sampling\n",
        "            probs = (visits ** (1 / temperature))\n",
        "            probs = probs / probs.sum()\n",
        "            for i, idx in enumerate(indices):\n",
        "                policy[idx] = probs[i]\n",
        "        \n",
        "        return policy\n",
        "    \n",
        "    def select_move(self, root: MCTSNode, temperature: float) -> chess.Move:\n",
        "        \"\"\"W\u00e4hle Zug basierend auf Visit-Counts\"\"\"\n",
        "        moves = list(root.children.keys())\n",
        "        visits = torch.tensor([root.children[m].visit_count for m in moves], dtype=torch.float32)\n",
        "        \n",
        "        if temperature <= 0.01:\n",
        "            return moves[visits.argmax()]\n",
        "        \n",
        "        probs = (visits ** (1 / temperature))\n",
        "        probs = probs / (probs.sum() + 1e-8)\n",
        "        idx = torch.multinomial(probs, 1).item()\n",
        "        return moves[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/atlas/selfplay.py\n",
        "import time\n",
        "import torch\n",
        "import chess\n",
        "from typing import List, Tuple, Dict\n",
        "from multiprocessing import Queue, Event\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.atlas.network import AtlasNetwork\n",
        "from prometheus_tqfd.atlas.mcts import MCTS\n",
        "from prometheus_tqfd.encoding import BoardEncoder, MoveEncoder\n",
        "\n",
        "class AtlasSelfPlayWorker:\n",
        "    \"\"\"\n",
        "    Generiert Self-Play-Spiele mit MCTS.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig, weights_queue: Queue, \n",
        "                 data_queue: Queue, device: str, worker_id: int):\n",
        "        self.config = config\n",
        "        self.weights_queue = weights_queue\n",
        "        self.data_queue = data_queue\n",
        "        self.device = device\n",
        "        self.worker_id = worker_id\n",
        "        \n",
        "        self.network = AtlasNetwork(config).to(device)\n",
        "        self.network.eval()\n",
        "        self.mcts = MCTS(config, self.network, device)\n",
        "        self.encoder = BoardEncoder()\n",
        "        self.move_encoder = MoveEncoder()\n",
        "        \n",
        "        self.games_played = 0\n",
        "        self.weights_version = 0\n",
        "    \n",
        "    def run(self, stop_event: Event, heartbeat_dict: Dict):\n",
        "        \"\"\"Hauptschleife des Workers\"\"\"\n",
        "        while not stop_event.is_set():\n",
        "            # Heartbeat\n",
        "            heartbeat_dict[f'atlas_selfplay_{self.worker_id}'] = time.time()\n",
        "            \n",
        "            # Gewichte updaten\n",
        "            self._maybe_update_weights()\n",
        "            \n",
        "            # Spiel spielen\n",
        "            trajectory = self._play_game()\n",
        "            \n",
        "            # In Queue schieben\n",
        "            self.data_queue.put(trajectory)\n",
        "            self.games_played += 1\n",
        "    \n",
        "    def _maybe_update_weights(self):\n",
        "        \"\"\"Lade neue Gewichte wenn verf\u00fcgbar\"\"\"\n",
        "        try:\n",
        "            # Get latest weights from queue\n",
        "            latest = None\n",
        "            while not self.weights_queue.empty():\n",
        "                latest = self.weights_queue.get_nowait()\n",
        "            \n",
        "            if latest:\n",
        "                weights, version = latest\n",
        "                if version > self.weights_version:\n",
        "                    self.network.load_state_dict(weights)\n",
        "                    self.weights_version = version\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    def _play_game(self) -> List[Tuple[torch.Tensor, torch.Tensor, float]]:\n",
        "        \"\"\"Spiele ein komplettes Spiel\"\"\"\n",
        "        trajectory = []\n",
        "        board = chess.Board()\n",
        "        move_count = 0\n",
        "        \n",
        "        while not board.is_game_over():\n",
        "            # Temperatur-Schedule\n",
        "            if move_count < self.config.atlas_temperature_moves:\n",
        "                temperature = self.config.atlas_temperature_init\n",
        "            else:\n",
        "                temperature = self.config.atlas_temperature_final\n",
        "            \n",
        "            # MCTS\n",
        "            root = self.mcts.search(board)\n",
        "            \n",
        "            # Daten sammeln\n",
        "            state_tensor = self.encoder.encode(board)\n",
        "            policy_target = self.mcts.get_policy_target(root, temperature)\n",
        "            \n",
        "            trajectory.append((state_tensor, policy_target, None))  # Value sp\u00e4ter\n",
        "            \n",
        "            # Zug ausf\u00fchren\n",
        "            move = self.mcts.select_move(root, temperature)\n",
        "            board.push(move)\n",
        "            move_count += 1\n",
        "            \n",
        "            # Spiell\u00e4nge begrenzen\n",
        "            if move_count > 400: # Slightly more than spec's 300 for safety\n",
        "                break\n",
        "        \n",
        "        # Value-Targets mit Spielergebnis f\u00fcllen\n",
        "        result = self._get_result(board)\n",
        "        final_trajectory = []\n",
        "        for i in range(len(trajectory)):\n",
        "            # Perspektive: Wei\u00df bei geraden Z\u00fcgen (0, 2...), Schwarz bei ungeraden (1, 3...)\n",
        "            # Wenn i=0, turn=White, perspective=1, value_target = result * 1\n",
        "            # Wenn result=1 (White wins), i=0 gets 1.\n",
        "            # Wenn i=1, turn=Black, perspective=-1, value_target = result * -1 = -1. Correct.\n",
        "            perspective = 1 if i % 2 == 0 else -1\n",
        "            value_target = result * perspective\n",
        "            final_trajectory.append((trajectory[i][0], trajectory[i][1], value_target))\n",
        "        \n",
        "        return final_trajectory\n",
        "    \n",
        "    def _get_result(self, board: chess.Board) -> float:\n",
        "        \"\"\"Spielergebnis aus Wei\u00df-Perspektive\"\"\"\n",
        "        result = board.result()\n",
        "        if result == \"1-0\":\n",
        "            return 1.0\n",
        "        elif result == \"0-1\":\n",
        "            return -1.0\n",
        "        return 0.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/atlas/trainer.py\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from multiprocessing import Queue, Event, Lock\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.atlas.network import AtlasNetwork\n",
        "from prometheus_tqfd.utils.replay_buffer import ReplayBuffer\n",
        "\n",
        "class AtlasTrainer:\n",
        "    \"\"\"\n",
        "    Trainiert das ATLAS-Netzwerk mit Daten aus Self-Play.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig, data_queue: Queue,\n",
        "                 weights_queue: Queue, device: str, shared_values: dict):\n",
        "        self.config = config\n",
        "        self.data_queue = data_queue\n",
        "        self.weights_queue = weights_queue\n",
        "        self.device = device\n",
        "        self.shared_values = shared_values\n",
        "        \n",
        "        self.network = AtlasNetwork(config).to(device)\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            self.network.parameters(),\n",
        "            lr=config.atlas_learning_rate,\n",
        "            weight_decay=config.atlas_weight_decay\n",
        "        )\n",
        "        self.scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
        "        \n",
        "        self.replay_buffer = ReplayBuffer(config.atlas_replay_size)\n",
        "        self.global_step = 0\n",
        "        self.weights_version = 0\n",
        "    \n",
        "    def run(self, stop_event: Event, pause_event: Event, \n",
        "            gpu_lock: Lock, heartbeat_dict: dict, metrics_queue: Queue):\n",
        "        \"\"\"Hauptschleife des Trainers\"\"\"\n",
        "        while not stop_event.is_set():\n",
        "            # Heartbeat\n",
        "            heartbeat_dict['atlas_trainer'] = time.time()\n",
        "            \n",
        "            # Pause pr\u00fcfen\n",
        "            if pause_event.is_set():\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "            \n",
        "            # Daten aus Queue holen\n",
        "            self._collect_data()\n",
        "            \n",
        "            # Training wenn genug Daten\n",
        "            if len(self.replay_buffer) >= self.config.min_buffer_before_training:\n",
        "                with gpu_lock:\n",
        "                    metrics = self._train_step()\n",
        "                    metrics_queue.put({\n",
        "                        'type': 'atlas_train',\n",
        "                        'step': self.global_step,\n",
        "                        **metrics\n",
        "                    })\n",
        "                \n",
        "                # Gewichte publishen\n",
        "                if self.global_step % self.config.weight_publish_interval == 0:\n",
        "                    self._publish_weights()\n",
        "            else:\n",
        "                time.sleep(1) # Wait for data\n",
        "    \n",
        "    def _collect_data(self):\n",
        "        \"\"\"Trajektorien aus Queue in Replay Buffer\"\"\"\n",
        "        try:\n",
        "            while not self.data_queue.empty():\n",
        "                trajectory = self.data_queue.get_nowait()\n",
        "                for state, policy, value in trajectory:\n",
        "                    self.replay_buffer.add(state, policy, value)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    def _train_step(self) -> dict:\n",
        "        \"\"\"Ein Trainingsschritt\"\"\"\n",
        "        self.network.train()\n",
        "        states, policies, values = self.replay_buffer.sample(self.config.atlas_batch_size)\n",
        "        states = states.to(self.device)\n",
        "        policies = policies.to(self.device)\n",
        "        values = values.to(self.device)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # Cast to device-appropriate autocast\n",
        "        device_type = 'cuda' if self.device == 'cuda' else 'cpu'\n",
        "        \n",
        "        with torch.autocast(device_type=device_type):\n",
        "            policy_logits, value_pred = self.network(states)\n",
        "            \n",
        "            # Policy Loss: Cross-Entropy\n",
        "            log_probs = F.log_softmax(policy_logits, dim=1)\n",
        "            policy_loss = -torch.sum(policies * log_probs, dim=1).mean()\n",
        "            \n",
        "            # Value Loss: MSE\n",
        "            value_loss = F.mse_loss(value_pred.squeeze(-1), values)\n",
        "            \n",
        "            # Total Loss\n",
        "            loss = policy_loss + value_loss\n",
        "        \n",
        "        if self.scaler:\n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.unscale_(self.optimizer)\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1.0)\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "        \n",
        "        self.global_step += 1\n",
        "        self.shared_values['atlas_steps'] = self.global_step\n",
        "        \n",
        "        return {\n",
        "            'loss': loss.item(),\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'value_loss': value_loss.item(),\n",
        "            'grad_norm': grad_norm.item() if grad_norm is not None else 0.0\n",
        "        }\n",
        "    \n",
        "    def _publish_weights(self):\n",
        "        \"\"\"Gewichte an Self-Play-Worker senden\"\"\"\n",
        "        self.weights_version += 1\n",
        "        weights = {k: v.cpu() for k, v in self.network.state_dict().items()}\n",
        "        \n",
        "        # Queue leeren und neue Gewichte einf\u00fcgen\n",
        "        try:\n",
        "            while not self.weights_queue.empty():\n",
        "                self.weights_queue.get_nowait()\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        self.weights_queue.put((weights, self.weights_version))\n",
        "        \n",
        "        # F\u00fcr Checkpoint\n",
        "        self.shared_values['atlas_weights'] = weights\n",
        "        self.shared_values['atlas_optimizer'] = self.optimizer.state_dict()\n",
        "        self.shared_values['atlas_version'] = self.weights_version\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/entropy/__init__.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/entropy/network.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.atlas.network import ResidualBlock\n",
        "\n",
        "class EntropyNetworkV2(nn.Module):\n",
        "    \"\"\"\n",
        "    Vereinfachtes CNN-only Netzwerk f\u00fcr ENTROPY v2.0.\n",
        "    \n",
        "    Input: 22 Kan\u00e4le (19 Board + 3 Physik-Felder)\n",
        "    Output:\n",
        "    - Policy Logits: [batch, 4672]\n",
        "    - Energy: [batch, 1] (unbeschr\u00e4nkt, aber mit Soft-Clipping)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig):\n",
        "        super().__init__()\n",
        "        C = config.entropy_channels\n",
        "        \n",
        "        # Input: 19 Board-Kan\u00e4le + 3 Physik-Feld-Kan\u00e4le\n",
        "        self.input_block = nn.Sequential(\n",
        "            nn.Conv2d(22, C, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(C),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Residual Tower (kleiner als ATLAS)\n",
        "        self.res_blocks = nn.ModuleList([\n",
        "            ResidualBlock(C) for _ in range(config.entropy_res_blocks)\n",
        "        ])\n",
        "        \n",
        "        # Policy Head\n",
        "        self.policy_head = nn.Sequential(\n",
        "            nn.Conv2d(C, 32, kernel_size=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 64, 4672)\n",
        "        )\n",
        "        \n",
        "        # Energy Head\n",
        "        self.energy_head = nn.Sequential(\n",
        "            nn.Conv2d(C, 4, kernel_size=1),\n",
        "            nn.BatchNorm2d(4),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(4 * 64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, board_tensor: torch.Tensor, \n",
        "                field_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Concatenate board + fields\n",
        "        x = torch.cat([board_tensor, field_tensor], dim=1)\n",
        "        \n",
        "        x = self.input_block(x)\n",
        "        for block in self.res_blocks:\n",
        "            x = block(x)\n",
        "        \n",
        "        policy_logits = self.policy_head(x)\n",
        "        energy = self.energy_head(x)\n",
        "        \n",
        "        # Soft-Clipping f\u00fcr Energie-Stabilit\u00e4t: tanh(x/10)*10\n",
        "        energy = torch.tanh(energy / 10.0) * 10.0\n",
        "        \n",
        "        return policy_logits, energy\n",
        "\n",
        "    def get_features(self, board_tensor, field_tensor):\n",
        "        \"\"\"Returns the flattened features before heads, for RND\"\"\"\n",
        "        x = torch.cat([board_tensor, field_tensor], dim=1)\n",
        "        x = self.input_block(x)\n",
        "        for block in self.res_blocks:\n",
        "            x = block(x)\n",
        "        return torch.flatten(x, 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/entropy/rollout.py\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import chess\n",
        "from typing import Tuple, Dict\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.entropy.network import EntropyNetworkV2\n",
        "from prometheus_tqfd.tactics import TacticsDetector\n",
        "from prometheus_tqfd.encoding import BoardEncoder, MoveEncoder\n",
        "from prometheus_tqfd.physics import PhysicsFieldCalculator\n",
        "\n",
        "class MiniRolloutSelector:\n",
        "    \"\"\"\n",
        "    Schaut 3-5 Z\u00fcge voraus mit policy-guided Rollouts.\n",
        "    Nutzt Energie als Evaluation statt klassischem Value.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig, network: EntropyNetworkV2,\n",
        "                 tactics: TacticsDetector, device: str):\n",
        "        self.config = config\n",
        "        self.network = network\n",
        "        self.tactics = tactics\n",
        "        self.device = device\n",
        "        \n",
        "        self.encoder = BoardEncoder()\n",
        "        self.field_calc = PhysicsFieldCalculator(config)\n",
        "        self.move_encoder = MoveEncoder()\n",
        "        \n",
        "        self.depth = config.entropy_rollout_depth\n",
        "        self.num_rollouts = config.entropy_rollout_count\n",
        "    \n",
        "    def select_move(self, board: chess.Board, temperature: float) -> Tuple[chess.Move, float]:\n",
        "        \"\"\"\n",
        "        W\u00e4hlt Zug basierend auf Mini-Rollouts.\n",
        "        \"\"\"\n",
        "        # Taktik-Check: Matt in 1 sofort spielen\n",
        "        threats = self.tactics.detect(board)\n",
        "        if threats['mate_in_1']:\n",
        "            return threats['mate_in_1'], 100.0\n",
        "        \n",
        "        legal_moves = list(board.legal_moves)\n",
        "        if not legal_moves:\n",
        "             return None, 0.0\n",
        "        if len(legal_moves) == 1:\n",
        "            return legal_moves[0], 0.0\n",
        "        \n",
        "        # Rollout-Scores f\u00fcr jeden Zug\n",
        "        move_scores = {}\n",
        "        for move in legal_moves:\n",
        "            scores = []\n",
        "            for _ in range(self.num_rollouts):\n",
        "                score = self._rollout(board, move, self.depth)\n",
        "                scores.append(score)\n",
        "            move_scores[move] = np.mean(scores)\n",
        "        \n",
        "        # Taktik-Boost addieren\n",
        "        tactic_boost = self.tactics.get_tactical_boost(board)\n",
        "        for move in legal_moves:\n",
        "            idx = self.move_encoder.move_to_index(move)\n",
        "            move_scores[move] += tactic_boost[idx].item() * 0.1\n",
        "        \n",
        "        # Boltzmann-Sampling\n",
        "        moves = list(move_scores.keys())\n",
        "        scores = torch.tensor([move_scores[m] for m in moves])\n",
        "        \n",
        "        if temperature <= 0.01:\n",
        "            chosen = moves[scores.argmax()]\n",
        "        else:\n",
        "            probs = F.softmax(scores / temperature, dim=0)\n",
        "            idx = torch.multinomial(probs, 1).item()\n",
        "            chosen = moves[idx]\n",
        "        \n",
        "        return chosen, move_scores[chosen]\n",
        "    \n",
        "    def _rollout(self, board: chess.Board, first_move: chess.Move, depth: int) -> float:\n",
        "        \"\"\"\n",
        "        Simuliert Spiel f\u00fcr 'depth' Z\u00fcge.\n",
        "        \"\"\"\n",
        "        sim_board = board.copy()\n",
        "        our_color = board.turn\n",
        "        sim_board.push(first_move)\n",
        "        \n",
        "        for d in range(depth - 1):\n",
        "            if sim_board.is_game_over():\n",
        "                return self._terminal_value(sim_board, our_color)\n",
        "            \n",
        "            # Schnelle Zug-Auswahl (Policy-Sampling ohne Rollout)\n",
        "            move = self._fast_select(sim_board)\n",
        "            sim_board.push(move)\n",
        "        \n",
        "        # Energie am Ende\n",
        "        energy = self._get_energy(sim_board)\n",
        "        \n",
        "        # Aus unserer Perspektive\n",
        "        # Wenn sim_board.turn == our_color, dann ist energy aus unserer sicht\n",
        "        # Aber die Energie wird vom Netz meist aus Sicht des aktuellen Spielers (oder absolut)\n",
        "        # In unserem Netz wird BoardEncoder benutzt, der Kanal 18 f\u00fcr \"turn\" hat.\n",
        "        # Wir m\u00fcssen sicherstellen, dass wir die Energie konsistent interpretieren.\n",
        "        # Im Spec steht: \"Aus unserer Perspektive: Wenn sim_board.turn != our_color: energy = -energy\"\n",
        "        if sim_board.turn != our_color:\n",
        "            energy = -energy\n",
        "        \n",
        "        return energy\n",
        "    \n",
        "    def _fast_select(self, board: chess.Board) -> chess.Move:\n",
        "        \"\"\"Schnelle Zug-Auswahl ohne Rollout\"\"\"\n",
        "        # Erst Taktik pr\u00fcfen\n",
        "        threats = self.tactics.detect(board)\n",
        "        if threats['mate_in_1']:\n",
        "            return threats['mate_in_1']\n",
        "        \n",
        "        # Sonst Policy-Sampling\n",
        "        board_tensor = self.encoder.encode(board).unsqueeze(0).to(self.device)\n",
        "        field_tensor = self.field_calc.compute(board).unsqueeze(0).to(self.device)\n",
        "        legal_mask = self.move_encoder.get_legal_mask(board).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            policy_logits, _ = self.network(board_tensor, field_tensor)\n",
        "        \n",
        "        policy_logits[~legal_mask.unsqueeze(0)] = float('-inf')\n",
        "        probs = F.softmax(policy_logits, dim=1).squeeze(0)\n",
        "        \n",
        "        idx = torch.multinomial(probs, 1).item()\n",
        "        return self.move_encoder.index_to_move(idx, board)\n",
        "    \n",
        "    def _get_energy(self, board: chess.Board) -> float:\n",
        "        \"\"\"Energie einer Position\"\"\"\n",
        "        board_tensor = self.encoder.encode(board).unsqueeze(0).to(self.device)\n",
        "        field_tensor = self.field_calc.compute(board).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            _, energy = self.network(board_tensor, field_tensor)\n",
        "        \n",
        "        return energy.item()\n",
        "    \n",
        "    def _terminal_value(self, board: chess.Board, our_color: chess.Color) -> float:\n",
        "        \"\"\"Wert einer Endstellung\"\"\"\n",
        "        result = board.result()\n",
        "        if result == \"1-0\":\n",
        "            return 10.0 if our_color == chess.WHITE else -10.0\n",
        "        elif result == \"0-1\":\n",
        "            return -10.0 if our_color == chess.WHITE else 10.0\n",
        "        return 0.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/entropy/loss.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Dict, Tuple\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "\n",
        "class EntropyV2Loss:\n",
        "    \"\"\"\n",
        "    Hybrid Loss-System f\u00fcr ENTROPY v2.0.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig, device: str):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.weights = {\n",
        "            'outcome': config.entropy_loss_outcome,\n",
        "            'mobility': config.entropy_loss_mobility,\n",
        "            'pressure': config.entropy_loss_pressure,\n",
        "            'stability': config.entropy_loss_stability,\n",
        "            'novelty': config.entropy_loss_novelty,\n",
        "        }\n",
        "        \n",
        "        # RND Networks\n",
        "        # Input features are C*64, where C is entropy_channels (128) -> 8192\n",
        "        # Wait, get_features returns flattened res_tower output.\n",
        "        # C=128, kernel 3, padding 1 keeps 8x8. So 128*8*8 = 8192.\n",
        "        self.feature_dim = config.entropy_channels * 64\n",
        "        self.rnd_target = self._make_rnd_net(frozen=True).to(device)\n",
        "        self.rnd_predictor = self._make_rnd_net(frozen=False).to(device)\n",
        "    \n",
        "    def _make_rnd_net(self, frozen: bool) -> nn.Module:\n",
        "        net = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "        )\n",
        "        if frozen:\n",
        "            for param in net.parameters():\n",
        "                param.requires_grad = False\n",
        "        return net\n",
        "    \n",
        "    def compute(self, batch: Dict, game_results: torch.Tensor) -> Tuple[torch.Tensor, Dict]:\n",
        "        \"\"\"\n",
        "        Berechnet den Gesamtverlust.\n",
        "        \"\"\"\n",
        "        losses = {}\n",
        "        \n",
        "        # 1. Outcome Loss (sparse)\n",
        "        losses['outcome'] = self._outcome_loss(batch['energy'], game_results)\n",
        "        \n",
        "        # 2. Mobility Loss\n",
        "        losses['mobility'] = self._mobility_loss(batch['policy_logits'], batch['legal_counts_self'])\n",
        "        \n",
        "        # 3. Pressure Loss\n",
        "        losses['pressure'] = self._pressure_loss(batch['legal_counts_self'], batch['legal_counts_opponent'])\n",
        "        \n",
        "        # 4. Stability Loss (TD)\n",
        "        losses['stability'] = self._stability_loss(batch['energy'], batch['energy_next'])\n",
        "        \n",
        "        # 5. Novelty Loss (RND)\n",
        "        losses['novelty'] = self._novelty_loss(batch['features'])\n",
        "        \n",
        "        # Gewichtete Summe\n",
        "        total = sum(self.weights[k] * losses[k] for k in losses)\n",
        "        \n",
        "        return total, {k: v.item() for k, v in losses.items()}\n",
        "    \n",
        "    def _outcome_loss(self, energy: torch.Tensor, results: torch.Tensor) -> torch.Tensor:\n",
        "        target = results.float().view(-1, 1) * 5.0  # Skalierung\n",
        "        return F.smooth_l1_loss(energy, target)\n",
        "    \n",
        "    def _mobility_loss(self, policy_logits: torch.Tensor, legal_counts: torch.Tensor) -> torch.Tensor:\n",
        "        probs = F.softmax(policy_logits, dim=1)\n",
        "        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
        "        \n",
        "        # Normalisiere mit Anzahl legaler Z\u00fcge\n",
        "        max_entropy = torch.log(legal_counts.float() + 1e-8)\n",
        "        normalized_entropy = entropy / (max_entropy + 1e-8)\n",
        "        \n",
        "        return -normalized_entropy.mean()\n",
        "    \n",
        "    def _pressure_loss(self, our_legal: torch.Tensor, opp_legal: torch.Tensor) -> torch.Tensor:\n",
        "        ratio = opp_legal.float() / (our_legal.float() + 1.0)\n",
        "        return ratio.mean()\n",
        "    \n",
        "    def _stability_loss(self, energy_now: torch.Tensor, energy_next: torch.Tensor) -> torch.Tensor:\n",
        "        gamma = 0.99\n",
        "        target = gamma * energy_next.detach()\n",
        "        return F.mse_loss(energy_now, target)\n",
        "    \n",
        "    def _novelty_loss(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        with torch.no_grad():\n",
        "            target_out = self.rnd_target(features)\n",
        "        predictor_out = self.rnd_predictor(features)\n",
        "        \n",
        "        error = ((target_out - predictor_out) ** 2).mean(dim=1)\n",
        "        return error.mean()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/entropy/selfplay.py\n",
        "import time\n",
        "import torch\n",
        "import chess\n",
        "from typing import List, Dict, Tuple\n",
        "from multiprocessing import Queue, Event\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.entropy.network import EntropyNetworkV2\n",
        "from prometheus_tqfd.entropy.rollout import MiniRolloutSelector\n",
        "from prometheus_tqfd.tactics import TacticsDetector\n",
        "from prometheus_tqfd.encoding import BoardEncoder, MoveEncoder\n",
        "from prometheus_tqfd.physics import PhysicsFieldCalculator\n",
        "\n",
        "class EntropySelfPlayWorker:\n",
        "    \"\"\"\n",
        "    Generiert Self-Play-Spiele mit Mini-Rollouts.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig, weights_queue: Queue,\n",
        "                 data_queue: Queue, device: str, worker_id: int):\n",
        "        self.config = config\n",
        "        self.weights_queue = weights_queue\n",
        "        self.data_queue = data_queue\n",
        "        self.device = device\n",
        "        self.worker_id = worker_id\n",
        "        \n",
        "        self.network = EntropyNetworkV2(config).to(device)\n",
        "        self.network.eval()\n",
        "        self.tactics = TacticsDetector(config)\n",
        "        self.selector = MiniRolloutSelector(config, self.network, self.tactics, device)\n",
        "        \n",
        "        self.encoder = BoardEncoder()\n",
        "        self.field_calc = PhysicsFieldCalculator(config)\n",
        "        self.move_encoder = MoveEncoder()\n",
        "        \n",
        "        self.temperature = config.entropy_temperature_start\n",
        "        self.games_played = 0\n",
        "        self.weights_version = 0\n",
        "    \n",
        "    def run(self, stop_event: Event, heartbeat_dict: Dict):\n",
        "        \"\"\"Hauptschleife\"\"\"\n",
        "        while not stop_event.is_set():\n",
        "            heartbeat_dict[f'entropy_selfplay_{self.worker_id}'] = time.time()\n",
        "            \n",
        "            self._maybe_update_weights()\n",
        "            trajectory = self._play_game()\n",
        "            self.data_queue.put(trajectory)\n",
        "            \n",
        "            self.games_played += 1\n",
        "            self._decay_temperature()\n",
        "    \n",
        "    def _play_game(self) -> List[Dict]:\n",
        "        \"\"\"Spiele ein komplettes Spiel\"\"\"\n",
        "        trajectory = []\n",
        "        board = chess.Board()\n",
        "        \n",
        "        while not board.is_game_over() and len(trajectory) < 400:\n",
        "            # Daten f\u00fcr diesen Zug\n",
        "            step_data = self._collect_step_data(board)\n",
        "            \n",
        "            # Zug w\u00e4hlen\n",
        "            move, energy_before = self.selector.select_move(board, self.temperature)\n",
        "            if move is None: break\n",
        "            \n",
        "            step_data['move_idx'] = self.move_encoder.move_to_index(move)\n",
        "            step_data['energy_before'] = energy_before\n",
        "            \n",
        "            # Zug ausf\u00fchren\n",
        "            board.push(move)\n",
        "            \n",
        "            # Energie nach Zug\n",
        "            step_data['energy_after'] = self._get_energy(board)\n",
        "            step_data['legal_count_opponent'] = len(list(board.legal_moves))\n",
        "            \n",
        "            trajectory.append(step_data)\n",
        "        \n",
        "        # Spielergebnis hinzuf\u00fcgen\n",
        "        result = self._get_result(board)\n",
        "        for i, step in enumerate(trajectory):\n",
        "            # Perspective: results are usually 1 for White win.\n",
        "            # If step i is White's turn (i=0, 2...), perspective is 1.\n",
        "            perspective = 1 if i % 2 == 0 else -1\n",
        "            step['game_result'] = result * perspective\n",
        "        \n",
        "        return trajectory\n",
        "    \n",
        "    def _collect_step_data(self, board: chess.Board) -> Dict:\n",
        "        \"\"\"Sammle alle Daten f\u00fcr einen Zug\"\"\"\n",
        "        board_tensor = self.encoder.encode(board)\n",
        "        field_tensor = self.field_calc.compute(board)\n",
        "        \n",
        "        # Features for RND\n",
        "        with torch.no_grad():\n",
        "            features = self.network.get_features(\n",
        "                board_tensor.unsqueeze(0).to(self.device),\n",
        "                field_tensor.unsqueeze(0).to(self.device)\n",
        "            ).squeeze(0).cpu()\n",
        "            \n",
        "            # Also need policy logits for mobility loss\n",
        "            policy_logits, _ = self.network(\n",
        "                board_tensor.unsqueeze(0).to(self.device),\n",
        "                field_tensor.unsqueeze(0).to(self.device)\n",
        "            )\n",
        "            policy_logits = policy_logits.squeeze(0).cpu()\n",
        "\n",
        "        return {\n",
        "            'board_tensor': board_tensor,\n",
        "            'field_tensor': field_tensor,\n",
        "            'features': features,\n",
        "            'policy_logits': policy_logits,\n",
        "            'legal_count_self': len(list(board.legal_moves)),\n",
        "        }\n",
        "    \n",
        "    def _get_energy(self, board: chess.Board) -> float:\n",
        "        board_tensor = self.encoder.encode(board).unsqueeze(0).to(self.device)\n",
        "        field_tensor = self.field_calc.compute(board).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            _, energy = self.network(board_tensor, field_tensor)\n",
        "        return energy.item()\n",
        "    \n",
        "    def _get_result(self, board: chess.Board) -> float:\n",
        "        result = board.result()\n",
        "        if result == \"1-0\":\n",
        "            return 1.0\n",
        "        elif result == \"0-1\":\n",
        "            return -1.0\n",
        "        return 0.0\n",
        "    \n",
        "    def _decay_temperature(self):\n",
        "        self.temperature = max(\n",
        "            self.config.entropy_temperature_end,\n",
        "            self.temperature * self.config.entropy_temperature_decay\n",
        "        )\n",
        "    \n",
        "    def _maybe_update_weights(self):\n",
        "        try:\n",
        "            latest = None\n",
        "            while not self.weights_queue.empty():\n",
        "                latest = self.weights_queue.get_nowait()\n",
        "            if latest:\n",
        "                weights, version = latest\n",
        "                if version > self.weights_version:\n",
        "                    self.network.load_state_dict(weights)\n",
        "                    self.weights_version = version\n",
        "        except:\n",
        "            pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/entropy/trainer.py\n",
        "import time\n",
        "import torch\n",
        "from multiprocessing import Queue, Event, Lock\n",
        "from typing import Dict\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.entropy.network import EntropyNetworkV2\n",
        "from prometheus_tqfd.entropy.loss import EntropyV2Loss\n",
        "from prometheus_tqfd.utils.replay_buffer import ReplayBuffer\n",
        "\n",
        "class EntropyTrainer:\n",
        "    \"\"\"\n",
        "    Trainiert das ENTROPY-Netzwerk.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig, data_queue: Queue,\n",
        "                 weights_queue: Queue, device: str, shared_values: dict):\n",
        "        self.config = config\n",
        "        self.data_queue = data_queue\n",
        "        self.weights_queue = weights_queue\n",
        "        self.device = device\n",
        "        self.shared_values = shared_values\n",
        "        \n",
        "        self.network = EntropyNetworkV2(config).to(device)\n",
        "        self.loss_fn = EntropyV2Loss(config, device)\n",
        "        \n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            list(self.network.parameters()) + list(self.loss_fn.rnd_predictor.parameters()),\n",
        "            lr=config.entropy_learning_rate\n",
        "        )\n",
        "        self.scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n",
        "        \n",
        "        self.replay_buffer = ReplayBuffer(config.entropy_replay_size)\n",
        "        self.global_step = 0\n",
        "        self.weights_version = 0\n",
        "    \n",
        "    def run(self, stop_event: Event, pause_event: Event, \n",
        "            gpu_lock: Lock, heartbeat_dict: dict, metrics_queue: Queue):\n",
        "        \"\"\"Hauptschleife\"\"\"\n",
        "        while not stop_event.is_set():\n",
        "            heartbeat_dict['entropy_trainer'] = time.time()\n",
        "            \n",
        "            if pause_event.is_set():\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "            \n",
        "            self._collect_data()\n",
        "            \n",
        "            if len(self.replay_buffer) >= self.config.min_buffer_before_training:\n",
        "                with gpu_lock:\n",
        "                    metrics = self._train_step()\n",
        "                    metrics_queue.put({\n",
        "                        'type': 'entropy_train',\n",
        "                        'step': self.global_step,\n",
        "                        **metrics\n",
        "                    })\n",
        "                \n",
        "                if self.global_step % self.config.weight_publish_interval == 0:\n",
        "                    self._publish_weights()\n",
        "            else:\n",
        "                time.sleep(1)\n",
        "    \n",
        "    def _collect_data(self):\n",
        "        try:\n",
        "            while not self.data_queue.empty():\n",
        "                trajectory = self.data_queue.get_nowait()\n",
        "                for step in trajectory:\n",
        "                    # We need to store (board, fields, features, policy_logits, legal_count_self, energy_after, legal_count_opponent, game_result)\n",
        "                    self.replay_buffer.add(\n",
        "                        (step['board_tensor'], step['field_tensor'], step['features'], step['policy_logits']),\n",
        "                        (step['legal_count_self'], step['legal_count_opponent'], step['energy_after']),\n",
        "                        step['game_result']\n",
        "                    )\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    def _train_step(self) -> dict:\n",
        "        self.network.train()\n",
        "        batch_size = self.config.entropy_batch_size\n",
        "        \n",
        "        states_batch = []\n",
        "        fields_batch = []\n",
        "        features_batch = []\n",
        "        policy_logits_batch = []\n",
        "        legal_self_batch = []\n",
        "        legal_opp_batch = []\n",
        "        energy_next_batch = []\n",
        "        results_batch = []\n",
        "        \n",
        "        data = self.replay_buffer.sample(batch_size)\n",
        "        # data is (states, policies, values) from ReplayBuffer. \n",
        "        # But for entropy we stored tuples in those positions.\n",
        "        \n",
        "        # Wait, the ReplayBuffer I wrote is:\n",
        "        # def add(self, state, policy, value):\n",
        "        #    self.buffer.append((state, policy, value))\n",
        "        # def sample(self, batch_size: int):\n",
        "        #    batch = random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
        "        #    states, policies, values = zip(*batch)\n",
        "        #    return (torch.stack(states), torch.stack(policies), torch.tensor(values))\n",
        "        \n",
        "        # In EntropyTrainer._collect_data:\n",
        "        # self.replay_buffer.add(\n",
        "        #     (step['board_tensor'], step['field_tensor'], step['features'], step['policy_logits']),\n",
        "        #     (step['legal_count_self'], step['legal_count_opponent'], step['energy_after']),\n",
        "        #     step['game_result']\n",
        "        # )\n",
        "        \n",
        "        # So we need to unpack.\n",
        "        samples = random.sample(self.replay_buffer.buffer, min(len(self.replay_buffer), batch_size))\n",
        "        \n",
        "        for (s_tup, p_tup, res) in samples:\n",
        "            states_batch.append(s_tup[0])\n",
        "            fields_batch.append(s_tup[1])\n",
        "            features_batch.append(s_tup[2])\n",
        "            policy_logits_batch.append(s_tup[3])\n",
        "            legal_self_batch.append(p_tup[0])\n",
        "            legal_opp_batch.append(p_tup[1])\n",
        "            energy_next_batch.append(p_tup[2])\n",
        "            results_batch.append(res)\n",
        "            \n",
        "        states = torch.stack(states_batch).to(self.device)\n",
        "        fields = torch.stack(fields_batch).to(self.device)\n",
        "        features = torch.stack(features_batch).to(self.device)\n",
        "        # policy_logits from buffer are precomputed, but we want the ones from the current model during training?\n",
        "        # Actually mobility loss uses policy_logits from current forward pass.\n",
        "        \n",
        "        energy_next = torch.tensor(energy_next_batch).float().to(self.device).view(-1, 1)\n",
        "        legal_self = torch.tensor(legal_self_batch).to(self.device)\n",
        "        legal_opp = torch.tensor(legal_opp_batch).to(self.device)\n",
        "        results = torch.tensor(results_batch).float().to(self.device)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        device_type = 'cuda' if self.device == 'cuda' else 'cpu'\n",
        "        \n",
        "        with torch.autocast(device_type=device_type):\n",
        "            policy_logits, energy = self.network(states, fields)\n",
        "            \n",
        "            batch_data = {\n",
        "                'states': states,\n",
        "                'policy_logits': policy_logits,\n",
        "                'energy': energy,\n",
        "                'energy_next': energy_next,\n",
        "                'legal_counts_self': legal_self,\n",
        "                'legal_counts_opponent': legal_opp,\n",
        "                'features': features\n",
        "            }\n",
        "            \n",
        "            loss, loss_dict = self.loss_fn.compute(batch_data, results)\n",
        "        \n",
        "        if self.scaler:\n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.unscale_(self.optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1.0)\n",
        "            self.scaler.step(self.optimizer)\n",
        "            self.scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.network.parameters(), 1.0)\n",
        "            self.optimizer.step()\n",
        "            \n",
        "        self.global_step += 1\n",
        "        self.shared_values['entropy_steps'] = self.global_step\n",
        "        \n",
        "        return {\n",
        "            'loss': loss.item(),\n",
        "            **loss_dict\n",
        "        }\n",
        "\n",
        "    def _publish_weights(self):\n",
        "        self.weights_version += 1\n",
        "        weights = {k: v.cpu() for k, v in self.network.state_dict().items()}\n",
        "        try:\n",
        "            while not self.weights_queue.empty():\n",
        "                self.weights_queue.get_nowait()\n",
        "        except:\n",
        "            pass\n",
        "        self.weights_queue.put((weights, self.weights_version))\n",
        "        self.shared_values['entropy_weights'] = weights\n",
        "        self.shared_values['entropy_optimizer'] = self.optimizer.state_dict()\n",
        "        self.shared_values['entropy_version'] = self.weights_version\n",
        "\n",
        "import random\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/evaluation/__init__.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/evaluation/arena.py\n",
        "import chess\n",
        "import torch\n",
        "import random\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.atlas.network import AtlasNetwork\n",
        "from prometheus_tqfd.atlas.mcts import MCTS\n",
        "from prometheus_tqfd.entropy.network import EntropyNetworkV2\n",
        "from prometheus_tqfd.entropy.rollout import MiniRolloutSelector\n",
        "from prometheus_tqfd.tactics import TacticsDetector\n",
        "from prometheus_tqfd.evaluation.baselines import RandomPlayer, HeuristicPlayer\n",
        "\n",
        "class AtlasArenaPlayer:\n",
        "    def __init__(self, config, network, device):\n",
        "        self.mcts = MCTS(config, network, device)\n",
        "    def select_move(self, board):\n",
        "        root = self.mcts.search(board, num_simulations=50) # Reduced for faster eval\n",
        "        return self.mcts.select_move(root, temperature=0)\n",
        "\n",
        "class EntropyArenaPlayer:\n",
        "    def __init__(self, config, network, device):\n",
        "        self.tactics = TacticsDetector(config)\n",
        "        self.selector = MiniRolloutSelector(config, network, self.tactics, device)\n",
        "    def select_move(self, board):\n",
        "        move, _ = self.selector.select_move(board, temperature=0)\n",
        "        return move\n",
        "\n",
        "class Arena:\n",
        "    \"\"\"\n",
        "    Veranstaltet Duelle zwischen Spielern und berechnet ELO.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig):\n",
        "        self.config = config\n",
        "        self.elo_ratings = {\n",
        "            'atlas': config.elo_initial,\n",
        "            'entropy': config.elo_initial,\n",
        "            'random': 400.0,\n",
        "            'heuristic': 800.0,\n",
        "        }\n",
        "        self.match_history = []\n",
        "    \n",
        "    def run_evaluation(self, atlas_network: AtlasNetwork, \n",
        "                       entropy_network: EntropyNetworkV2,\n",
        "                       device: str) -> Dict:\n",
        "        \"\"\"Komplette Evaluationsrunde\"\"\"\n",
        "        atlas_network.eval()\n",
        "        entropy_network.eval()\n",
        "        \n",
        "        results = {}\n",
        "        \n",
        "        # Players\n",
        "        atlas_player = AtlasArenaPlayer(self.config, atlas_network, device)\n",
        "        entropy_player = EntropyArenaPlayer(self.config, entropy_network, device)\n",
        "        random_player = RandomPlayer()\n",
        "        heuristic_player = HeuristicPlayer()\n",
        "        \n",
        "        # 1. ATLAS vs ENTROPY\n",
        "        wins_a, wins_e, draws = self._play_match(atlas_player, entropy_player, self.config.eval_games_atlas_entropy)\n",
        "        results['atlas_vs_entropy'] = {'atlas': wins_a, 'entropy': wins_e, 'draws': draws}\n",
        "        self._update_elo('atlas', 'entropy', wins_a, wins_e, draws)\n",
        "        \n",
        "        # 2. vs Baselines\n",
        "        for name, player in [('atlas', atlas_player), ('entropy', entropy_player)]:\n",
        "            # vs Random\n",
        "            w, l, d = self._play_match(player, random_player, self.config.eval_games_vs_random)\n",
        "            results[f'{name}_vs_random'] = {'wins': w, 'losses': l, 'draws': d}\n",
        "            self._update_elo(name, 'random', w, l, d, update_b=False)\n",
        "            \n",
        "            # vs Heuristic\n",
        "            w, l, d = self._play_match(player, heuristic_player, self.config.eval_games_vs_heuristic)\n",
        "            results[f'{name}_vs_heuristic'] = {'wins': w, 'losses': l, 'draws': d}\n",
        "            self._update_elo(name, 'heuristic', w, l, d, update_b=False)\n",
        "        \n",
        "        results['elo'] = self.elo_ratings.copy()\n",
        "        return results\n",
        "    \n",
        "    def _play_match(self, p1, p2, num_games: int) -> Tuple[int, int, int]:\n",
        "        wins1, wins2, draws = 0, 0, 0\n",
        "        for i in range(num_games):\n",
        "            # Alternate colors\n",
        "            if i % 2 == 0:\n",
        "                res = self._play_game(p1, p2)\n",
        "                if res == 1.0: wins1 += 1\n",
        "                elif res == -1.0: wins2 += 1\n",
        "                else: draws += 1\n",
        "            else:\n",
        "                res = self._play_game(p2, p1)\n",
        "                if res == 1.0: wins2 += 1\n",
        "                elif res == -1.0: wins1 += 1\n",
        "                else: draws += 1\n",
        "        return wins1, wins2, draws\n",
        "    \n",
        "    def _play_game(self, white_player, black_player, max_moves: int = 200) -> float:\n",
        "        board = chess.Board()\n",
        "        while not board.is_game_over() and board.fullmove_number <= max_moves:\n",
        "            player = white_player if board.turn == chess.WHITE else black_player\n",
        "            move = player.select_move(board)\n",
        "            if move is None or move not in board.legal_moves:\n",
        "                # Fallback to random if player fails\n",
        "                move = random.choice(list(board.legal_moves))\n",
        "            board.push(move)\n",
        "        \n",
        "        res = board.result()\n",
        "        if res == \"1-0\": return 1.0\n",
        "        if res == \"0-1\": return -1.0\n",
        "        return 0.0\n",
        "    \n",
        "    def _update_elo(self, p_a: str, p_b: str, wins_a: int, wins_b: int, draws: int, update_b: bool = True):\n",
        "        total = wins_a + wins_b + draws\n",
        "        if total == 0: return\n",
        "        \n",
        "        ra = self.elo_ratings[p_a]\n",
        "        rb = self.elo_ratings[p_b]\n",
        "        \n",
        "        ea = 1 / (1 + 10 ** ((rb - ra) / 400))\n",
        "        sa = (wins_a + 0.5 * draws) / total\n",
        "        \n",
        "        k = self.config.elo_k_factor\n",
        "        self.elo_ratings[p_a] = ra + k * (sa - ea)\n",
        "        if update_b:\n",
        "            eb = 1 - ea\n",
        "            sb = 1 - sa\n",
        "            self.elo_ratings[p_b] = rb + k * (sb - eb)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/evaluation/baselines.py\n",
        "import random\n",
        "import chess\n",
        "\n",
        "class RandomPlayer:\n",
        "    \"\"\"W\u00e4hlt uniform zuf\u00e4llig aus legalen Z\u00fcgen\"\"\"\n",
        "    def select_move(self, board: chess.Board) -> chess.Move:\n",
        "        return random.choice(list(board.legal_moves))\n",
        "\n",
        "class HeuristicPlayer:\n",
        "    \"\"\"\n",
        "    Einfache regelbasierte Heuristik:\n",
        "    - Materialbewertung\n",
        "    - Zentrumskontrolle\n",
        "    - Mobilit\u00e4t\n",
        "    - K\u00f6nigssicherheit\n",
        "    \"\"\"\n",
        "    \n",
        "    PIECE_VALUES = {\n",
        "        chess.PAWN: 100, chess.KNIGHT: 320, chess.BISHOP: 330,\n",
        "        chess.ROOK: 500, chess.QUEEN: 900, chess.KING: 0\n",
        "    }\n",
        "    \n",
        "    CENTER_SQUARES = [chess.D4, chess.D5, chess.E4, chess.E5]\n",
        "    \n",
        "    def select_move(self, board: chess.Board) -> chess.Move:\n",
        "        legal_moves = list(board.legal_moves)\n",
        "        if not legal_moves: return None\n",
        "        \n",
        "        # 10% Exploration\n",
        "        if random.random() < 0.1:\n",
        "            return random.choice(legal_moves)\n",
        "        \n",
        "        best_move = None\n",
        "        best_score = float('-inf')\n",
        "        \n",
        "        for move in legal_moves:\n",
        "            board.push(move)\n",
        "            score = self._evaluate(board)\n",
        "            board.pop()\n",
        "            \n",
        "            # Perspektive: Nach unserem Zug ist Gegner dran\n",
        "            # evaluate gibt wert f\u00fcr Wei\u00df zur\u00fcck. Wenn wir Schwarz sind, wollen wir niedrigen Wert.\n",
        "            # Aber hier machen wir es einfacher: _evaluate gibt wert f\u00fcr Spieler am Zug zur\u00fcck?\n",
        "            # Nein, _evaluate gibt absoluten Wert (Wei\u00df positiv).\n",
        "            # Wenn wir am Zug sind und Wei\u00df sind, wollen wir max score.\n",
        "            # Wenn wir am Zug sind und Schwarz sind, wollen wir min score.\n",
        "            \n",
        "            actual_score = score if board.turn == chess.WHITE else -score\n",
        "            \n",
        "            if actual_score > best_score:\n",
        "                best_score = actual_score\n",
        "                best_move = move\n",
        "        \n",
        "        return best_move or random.choice(legal_moves)\n",
        "    \n",
        "    def _evaluate(self, board: chess.Board) -> float:\n",
        "        if board.is_checkmate():\n",
        "            return -10000 if board.turn == chess.WHITE else 10000\n",
        "        if board.is_stalemate() or board.is_insufficient_material():\n",
        "            return 0\n",
        "        \n",
        "        score = 0\n",
        "        \n",
        "        # Material\n",
        "        for square in chess.SQUARES:\n",
        "            piece = board.piece_at(square)\n",
        "            if piece:\n",
        "                value = self.PIECE_VALUES[piece.piece_type]\n",
        "                score += value if piece.color == chess.WHITE else -value\n",
        "        \n",
        "        # Zentrumskontrolle\n",
        "        for sq in self.CENTER_SQUARES:\n",
        "            if board.is_attacked_by(chess.WHITE, sq):\n",
        "                score += 10\n",
        "            if board.is_attacked_by(chess.BLACK, sq):\n",
        "                score -= 10\n",
        "        \n",
        "        # Mobilit\u00e4t (approximiert durch legal moves)\n",
        "        # Vorsicht: board.turn \u00e4ndern ist gef\u00e4hrlich w\u00e4hrend iteration\n",
        "        original_turn = board.turn\n",
        "        board.turn = chess.WHITE\n",
        "        score += len(list(board.legal_moves)) * 2\n",
        "        board.turn = chess.BLACK\n",
        "        score -= len(list(board.legal_moves)) * 2\n",
        "        board.turn = original_turn\n",
        "        \n",
        "        return score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/orchestration/__init__.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/orchestration/checkpoint.py\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "import lz4.frame\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Optional, Dict\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "\n",
        "class CheckpointManager:\n",
        "    \"\"\"\n",
        "    Tiered Checkpoint-System: Micro (5min), Light (15min), Full (60min).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig):\n",
        "        self.config = config\n",
        "        self.base_path = config.base_dir / config.run_id / 'checkpoints'\n",
        "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # drive path if used\n",
        "        self.drive_path = Path('/content/drive/MyDrive/prometheus_chess') / config.run_id / 'checkpoints' if config.use_drive else None\n",
        "        if self.drive_path:\n",
        "            self.drive_path.mkdir(parents=True, exist_ok=True)\n",
        "            \n",
        "        self.last_micro = time.time()\n",
        "        self.last_light = time.time()\n",
        "        self.last_full = time.time()\n",
        "    \n",
        "    def maybe_checkpoint(self, shared_values: dict, replay_buffers: dict = None):\n",
        "        now = time.time()\n",
        "        \n",
        "        if now - self.last_full > self.config.checkpoint_full_interval * 60:\n",
        "            self.save_full(shared_values, replay_buffers)\n",
        "            self.last_full = now\n",
        "            self.last_light = now\n",
        "            self.last_micro = now\n",
        "        elif now - self.last_light > self.config.checkpoint_light_interval * 60:\n",
        "            self.save_light(shared_values)\n",
        "            self.last_light = now\n",
        "            self.last_micro = now\n",
        "        elif now - self.last_micro > self.config.checkpoint_micro_interval * 60:\n",
        "            self.save_micro(shared_values)\n",
        "            self.last_micro = now\n",
        "            \n",
        "    def save_micro(self, shared_values: dict):\n",
        "        path = self.base_path / 'latest'\n",
        "        path.mkdir(exist_ok=True)\n",
        "        \n",
        "        if 'atlas_weights' in shared_values:\n",
        "            torch.save(shared_values['atlas_weights'], path / 'atlas_weights.pt')\n",
        "        if 'entropy_weights' in shared_values:\n",
        "            torch.save(shared_values['entropy_weights'], path / 'entropy_weights.pt')\n",
        "            \n",
        "        metadata = {\n",
        "            'type': 'micro',\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'atlas_steps': shared_values.get('atlas_steps', 0),\n",
        "            'entropy_steps': shared_values.get('entropy_steps', 0),\n",
        "            'atlas_version': shared_values.get('atlas_version', 0),\n",
        "            'entropy_version': shared_values.get('entropy_version', 0),\n",
        "        }\n",
        "        with open(path / 'metadata.json', 'w') as f:\n",
        "            json.dump(metadata, f)\n",
        "\n",
        "    def save_light(self, shared_values: dict):\n",
        "        self.save_micro(shared_values)\n",
        "        path = self.base_path / 'latest'\n",
        "        \n",
        "        if 'atlas_optimizer' in shared_values:\n",
        "            torch.save(shared_values['atlas_optimizer'], path / 'atlas_optimizer.pt')\n",
        "        if 'entropy_optimizer' in shared_values:\n",
        "            torch.save(shared_values['entropy_optimizer'], path / 'entropy_optimizer.pt')\n",
        "            \n",
        "        rng_states = {\n",
        "            'python': random.getstate(),\n",
        "            'numpy': np.random.get_state(),\n",
        "            'torch': torch.get_rng_state(),\n",
        "        }\n",
        "        if torch.cuda.is_available():\n",
        "            rng_states['cuda'] = torch.cuda.get_rng_state()\n",
        "        torch.save(rng_states, path / 'rng_states.pt')\n",
        "        \n",
        "        with open(path / 'metadata.json', 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        metadata['type'] = 'light'\n",
        "        with open(path / 'metadata.json', 'w') as f:\n",
        "            json.dump(metadata, f)\n",
        "\n",
        "    def save_full(self, shared_values: dict, replay_buffers: dict = None):\n",
        "        self.save_light(shared_values)\n",
        "        path = self.base_path / 'latest'\n",
        "        \n",
        "        if replay_buffers:\n",
        "            for name, buffer in replay_buffers.items():\n",
        "                with lz4.frame.open(path / f'{name}_replay.lz4', 'wb') as f:\n",
        "                    pickle.dump(buffer.get_data(), f)\n",
        "                    \n",
        "        with open(path / 'metadata.json', 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "        metadata['type'] = 'full'\n",
        "        with open(path / 'metadata.json', 'w') as f:\n",
        "            json.dump(metadata, f)\n",
        "            \n",
        "        # Archive current 'latest' to a timestamped folder\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        archive_path = self.base_path / f'full_{timestamp}'\n",
        "        import shutil\n",
        "        shutil.copytree(path, archive_path)\n",
        "        \n",
        "        # Sync to drive if enabled\n",
        "        if self.drive_path:\n",
        "            drive_latest = self.drive_path / 'latest'\n",
        "            if drive_latest.exists(): shutil.rmtree(drive_latest)\n",
        "            shutil.copytree(path, drive_latest)\n",
        "            print(f\"\ud83d\udcbe Checkpoint mirrored to Google Drive\")\n",
        "\n",
        "    def load_latest(self) -> Optional[dict]:\n",
        "        path = self.base_path / 'latest'\n",
        "        # If not local, check drive\n",
        "        if not path.exists() and self.drive_path:\n",
        "            drive_path = self.drive_path / 'latest'\n",
        "            if drive_path.exists():\n",
        "                import shutil\n",
        "                shutil.copytree(drive_path, path)\n",
        "                print(\"\ud83d\udcc2 Restored checkpoint from Google Drive\")\n",
        "                \n",
        "        if not path.exists():\n",
        "            return None\n",
        "            \n",
        "        data = {}\n",
        "        try:\n",
        "            if (path / 'atlas_weights.pt').exists():\n",
        "                data['atlas_weights'] = torch.load(path / 'atlas_weights.pt')\n",
        "            if (path / 'entropy_weights.pt').exists():\n",
        "                data['entropy_weights'] = torch.load(path / 'entropy_weights.pt')\n",
        "            if (path / 'atlas_optimizer.pt').exists():\n",
        "                data['atlas_optimizer'] = torch.load(path / 'atlas_optimizer.pt')\n",
        "            if (path / 'entropy_optimizer.pt').exists():\n",
        "                data['entropy_optimizer'] = torch.load(path / 'entropy_optimizer.pt')\n",
        "            if (path / 'rng_states.pt').exists():\n",
        "                data['rng_states'] = torch.load(path / 'rng_states.pt')\n",
        "            if (path / 'metadata.json').exists():\n",
        "                with open(path / 'metadata.json', 'r') as f:\n",
        "                    data['metadata'] = json.load(f)\n",
        "            return data\n",
        "        except Exception as e:\n",
        "            print(f\"\u26a0\ufe0f Error loading checkpoint: {e}\")\n",
        "            return None\n",
        "            \n",
        "    def load_replay_buffer(self, name: str) -> Optional[list]:\n",
        "        path = self.base_path / 'latest' / f'{name}_replay.lz4'\n",
        "        if path.exists():\n",
        "            with lz4.frame.open(path, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/orchestration/recovery.py\n",
        "import torch\n",
        "import gc\n",
        "import time\n",
        "from multiprocessing import Event\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.orchestration.checkpoint import CheckpointManager\n",
        "\n",
        "class OOMHandler:\n",
        "    \"\"\"\n",
        "    Behandelt Out-of-Memory Situationen.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig):\n",
        "        self.config = config\n",
        "        self.current_batch_size_atlas = config.atlas_batch_size\n",
        "        self.current_batch_size_entropy = config.entropy_batch_size\n",
        "        self.oom_count = 0\n",
        "    \n",
        "    def handle_oom(self, process_name: str, checkpoint_manager: CheckpointManager,\n",
        "                   shared_values: dict, pause_event: Event):\n",
        "        self.oom_count += 1\n",
        "        print(f\"\u26a0\ufe0f OOM #{self.oom_count} in {process_name}\")\n",
        "        \n",
        "        # 1. Pause setzen\n",
        "        pause_event.set()\n",
        "        \n",
        "        # 2. GPU-Speicher freigeben\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        \n",
        "        # 3. Batch-Size reduzieren\n",
        "        self.current_batch_size_atlas = max(\n",
        "            self.config.oom_min_batch_size,\n",
        "            int(self.current_batch_size_atlas * self.config.oom_batch_reduction)\n",
        "        )\n",
        "        self.current_batch_size_entropy = max(\n",
        "            self.config.oom_min_batch_size,\n",
        "            int(self.current_batch_size_entropy * self.config.oom_batch_reduction)\n",
        "        )\n",
        "        \n",
        "        print(f\"   Reduced batch sizes: ATLAS={self.current_batch_size_atlas}, ENTROPY={self.current_batch_size_entropy}\")\n",
        "        \n",
        "        # 4. Wait\n",
        "        time.sleep(5)\n",
        "        \n",
        "        # 5. Load latest checkpoint to shared state\n",
        "        checkpoint = checkpoint_manager.load_latest()\n",
        "        if checkpoint:\n",
        "            shared_values.update(checkpoint)\n",
        "        \n",
        "        # 6. Resume\n",
        "        pause_event.clear()\n",
        "        print(f\"\u2705 System resumed after OOM.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/orchestration/supervisor.py\n",
        "import time\n",
        "import os\n",
        "import signal\n",
        "import multiprocessing as mp\n",
        "from typing import Dict, List\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.orchestration.checkpoint import CheckpointManager\n",
        "from prometheus_tqfd.orchestration.recovery import OOMHandler\n",
        "from prometheus_tqfd.evaluation.arena import Arena\n",
        "\n",
        "class Supervisor:\n",
        "    \"\"\"\n",
        "    Hauptorchestrierer des Systems.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config: PrometheusConfig):\n",
        "        self.config = config\n",
        "        self.device = 'cuda' if torch_is_cuda() else 'cpu'\n",
        "        \n",
        "        # Shared State\n",
        "        self.manager = mp.Manager()\n",
        "        self.shared_values = self.manager.dict()\n",
        "        self.heartbeats = self.manager.dict()\n",
        "        \n",
        "        # Events\n",
        "        self.stop_event = mp.Event()\n",
        "        self.pause_event = mp.Event()\n",
        "        \n",
        "        # Locks\n",
        "        self.gpu_lock = mp.Lock()\n",
        "        \n",
        "        # Queues\n",
        "        self.atlas_data_queue = mp.Queue(maxsize=100)\n",
        "        self.atlas_weights_queue = mp.Queue(maxsize=1)\n",
        "        self.entropy_data_queue = mp.Queue(maxsize=100)\n",
        "        self.entropy_weights_queue = mp.Queue(maxsize=1)\n",
        "        self.metrics_queue = mp.Queue(maxsize=10000)\n",
        "        \n",
        "        # Components\n",
        "        self.checkpoint_manager = CheckpointManager(config)\n",
        "        self.oom_handler = OOMHandler(config)\n",
        "        self.arena = Arena(config)\n",
        "        from prometheus_tqfd.utils.logging import MetricsLogger\n",
        "        self.metrics_logger = MetricsLogger(config)\n",
        "        \n",
        "        # Processes\n",
        "        self.processes = {}\n",
        "        \n",
        "    def start(self):\n",
        "        \"\"\"Startet alle Prozesse\"\"\"\n",
        "        # Trainers\n",
        "        self._start_process('atlas_trainer', self._run_atlas_trainer)\n",
        "        self._start_process('entropy_trainer', self._run_entropy_trainer)\n",
        "        \n",
        "        # Self-Play Workers\n",
        "        for i in range(self.config.num_atlas_selfplay_workers):\n",
        "            self._start_process(f'atlas_selfplay_{i}', lambda i=i: self._run_atlas_selfplay(i))\n",
        "            \n",
        "        for i in range(self.config.num_entropy_selfplay_workers):\n",
        "            self._start_process(f'entropy_selfplay_{i}', lambda i=i: self._run_entropy_selfplay(i))\n",
        "            \n",
        "    def run(self):\n",
        "        \"\"\"Hauptschleife\"\"\"\n",
        "        self.start()\n",
        "        last_eval = 0\n",
        "        \n",
        "        try:\n",
        "            while not self.stop_event.is_set():\n",
        "                # 1. Heartbeats pr\u00fcfen\n",
        "                self._check_heartbeats()\n",
        "                \n",
        "                # 2. Metrics sammeln und loggen (DRAIN QUEUE)\n",
        "                self._collect_metrics()\n",
        "                \n",
        "                # 3. Checkpoints\n",
        "                self.checkpoint_manager.maybe_checkpoint(dict(self.shared_values))\n",
        "                \n",
        "                # 4. Evaluation\n",
        "                atlas_steps = self.shared_values.get('atlas_steps', 0)\n",
        "                entropy_steps = self.shared_values.get('entropy_steps', 0)\n",
        "                total_steps = atlas_steps + entropy_steps\n",
        "                \n",
        "                if total_steps - last_eval >= self.config.eval_interval_games:\n",
        "                    self._run_evaluation()\n",
        "                    last_eval = total_steps\n",
        "                \n",
        "                time.sleep(5)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\ud83d\uded1 Supervisor: Shutdown requested...\")\n",
        "        finally:\n",
        "            self.stop()\n",
        "            \n",
        "    def stop(self):\n",
        "        self.stop_event.set()\n",
        "        print(\"\ud83d\udcbe Saving final checkpoint...\")\n",
        "        self.checkpoint_manager.save_full(dict(self.shared_values))\n",
        "        \n",
        "        print(\"\ud83e\uddf9 Terminating processes...\")\n",
        "        for name, proc in self.processes.items():\n",
        "            if proc.is_alive():\n",
        "                proc.terminate()\n",
        "                proc.join(timeout=5)\n",
        "        print(\"\u2705 Supervisor: Shutdown complete.\")\n",
        "\n",
        "    def _start_process(self, name: str, target_fn):\n",
        "        p = mp.Process(target=target_fn, name=name, daemon=True)\n",
        "        p.start()\n",
        "        self.processes[name] = p\n",
        "        print(f\"\ud83d\ude80 Started {name} (PID: {p.pid})\")\n",
        "\n",
        "    def _check_heartbeats(self):\n",
        "        now = time.time()\n",
        "        for name, last_beat in dict(self.heartbeats).items():\n",
        "            if now - last_beat > self.config.heartbeat_timeout:\n",
        "                print(f\"\u26a0\ufe0f {name} timed out, restarting...\")\n",
        "                self._restart_process(name)\n",
        "\n",
        "    def _collect_metrics(self):\n",
        "        \"\"\"Drains the metrics queue and logs to file.\"\"\"\n",
        "        try:\n",
        "            while not self.metrics_queue.empty():\n",
        "                m = self.metrics_queue.get_nowait()\n",
        "                self.metrics_logger.log(m)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def _run_evaluation(self):\n",
        "        print(\"\u2694\ufe0f Running Arena Evaluation...\")\n",
        "        # We need actual models here. This is slightly tricky in the supervisor process.\n",
        "        # For simplicity in this script, we'll log that we're doing it.\n",
        "        # In a full implementation, we'd load weights into networks.\n",
        "        from prometheus_tqfd.atlas.network import AtlasNetwork\n",
        "        from prometheus_tqfd.entropy.network import EntropyNetworkV2\n",
        "        \n",
        "        atlas_net = AtlasNetwork(self.config).to('cpu')\n",
        "        entropy_net = EntropyNetworkV2(self.config).to('cpu')\n",
        "        \n",
        "        if 'atlas_weights' in self.shared_values:\n",
        "            atlas_net.load_state_dict(self.shared_values['atlas_weights'])\n",
        "        if 'entropy_weights' in self.shared_values:\n",
        "            entropy_net.load_state_dict(self.shared_values['entropy_weights'])\n",
        "            \n",
        "        results = self.arena.run_evaluation(atlas_net, entropy_net, 'cpu')\n",
        "        results['type'] = 'evaluation'\n",
        "        self.metrics_logger.log(results)\n",
        "        \n",
        "        # Update shared ELO\n",
        "        for k, v in results['elo'].items():\n",
        "            self.shared_values[f'elo_{k}'] = v\n",
        "        print(f\"\ud83d\udcca New ELOs: {results['elo']}\")\n",
        "\n",
        "    def _restart_process(self, name: str):\n",
        "        if name in self.processes:\n",
        "            p = self.processes[name]\n",
        "            if p.is_alive():\n",
        "                p.terminate()\n",
        "                p.join(timeout=2)\n",
        "        \n",
        "        if 'atlas_trainer' in name:\n",
        "            self._start_process(name, self._run_atlas_trainer)\n",
        "        elif 'entropy_trainer' in name:\n",
        "            self._start_process(name, self._run_entropy_trainer)\n",
        "        elif 'atlas_selfplay' in name:\n",
        "            i = int(name.split('_')[-1])\n",
        "            self._start_process(name, lambda: self._run_atlas_selfplay(i))\n",
        "        elif 'entropy_selfplay' in name:\n",
        "            i = int(name.split('_')[-1])\n",
        "            self._start_process(name, lambda: self._run_entropy_selfplay(i))\n",
        "\n",
        "    # Runner functions that instantiate the components in the subprocesses\n",
        "    def _run_atlas_trainer(self):\n",
        "        from prometheus_tqfd.atlas.trainer import AtlasTrainer\n",
        "        trainer = AtlasTrainer(self.config, self.atlas_data_queue, self.atlas_weights_queue, self.device, self.shared_values)\n",
        "        try:\n",
        "            trainer.run(self.stop_event, self.pause_event, self.gpu_lock, self.heartbeats, self.metrics_queue)\n",
        "        except Exception as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                self.oom_handler.handle_oom('atlas_trainer', self.checkpoint_manager, self.shared_values, self.pause_event)\n",
        "\n",
        "    def _run_entropy_trainer(self):\n",
        "        from prometheus_tqfd.entropy.trainer import EntropyTrainer\n",
        "        trainer = EntropyTrainer(self.config, self.entropy_data_queue, self.entropy_weights_queue, self.device, self.shared_values)\n",
        "        try:\n",
        "            trainer.run(self.stop_event, self.pause_event, self.gpu_lock, self.heartbeats, self.metrics_queue)\n",
        "        except Exception as e:\n",
        "            if \"out of memory\" in str(e).lower():\n",
        "                self.oom_handler.handle_oom('entropy_trainer', self.checkpoint_manager, self.shared_values, self.pause_event)\n",
        "\n",
        "    def _run_atlas_selfplay(self, i: int):\n",
        "        from prometheus_tqfd.atlas.selfplay import AtlasSelfPlayWorker\n",
        "        # Self-play often on CPU for stability or lower GPU memory\n",
        "        worker = AtlasSelfPlayWorker(self.config, self.atlas_weights_queue, self.atlas_data_queue, 'cpu', i)\n",
        "        worker.run(self.stop_event, self.heartbeats)\n",
        "\n",
        "    def _run_entropy_selfplay(self, i: int):\n",
        "        from prometheus_tqfd.entropy.selfplay import EntropySelfPlayWorker\n",
        "        worker = EntropySelfPlayWorker(self.config, self.entropy_weights_queue, self.entropy_data_queue, 'cpu', i)\n",
        "        worker.run(self.stop_event, self.heartbeats)\n",
        "\n",
        "def torch_is_cuda():\n",
        "    import torch\n",
        "    return torch.cuda.is_available()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/utils/__init__.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/utils/hardware.py\n",
        "import torch\n",
        "import psutil\n",
        "from prometheus_tqfd.config import HardwareConfig\n",
        "\n",
        "def detect_hardware() -> HardwareConfig:\n",
        "    \"\"\"\n",
        "    Erkennt Hardware und setzt optimale Parameter.\n",
        "    \"\"\"\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    gpu_name = None\n",
        "    vram_gb = None\n",
        "    \n",
        "    if device == 'cuda':\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "\n",
        "    ram_gb = psutil.virtual_memory().total / (1024**3)\n",
        "    cpu_cores = psutil.cpu_count()\n",
        "    \n",
        "    # Colab Check\n",
        "    is_colab = False\n",
        "    try:\n",
        "        import google.colab\n",
        "        is_colab = True\n",
        "    except ImportError:\n",
        "        pass\n",
        "        \n",
        "    return HardwareConfig(\n",
        "        device=device,\n",
        "        gpu_name=gpu_name,\n",
        "        vram_gb=vram_gb,\n",
        "        ram_gb=ram_gb,\n",
        "        cpu_cores=cpu_cores,\n",
        "        is_colab=is_colab\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/utils/logging.py\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "class MetricsLogger:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.log_file = config.base_dir / config.run_id / 'metrics' / 'metrics.jsonl'\n",
        "        self.log_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    def log(self, metrics):\n",
        "        metrics['timestamp'] = time.time()\n",
        "        with open(self.log_file, 'a') as f:\n",
        "            f.write(json.dumps(metrics) + '\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/utils/replay_buffer.py\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    \n",
        "    def add(self, state, policy, value):\n",
        "        self.buffer.append((state, policy, value))\n",
        "    \n",
        "    def sample(self, batch_size: int):\n",
        "        batch = random.sample(self.buffer, min(len(self.buffer), batch_size))\n",
        "        states, policies, values = zip(*batch)\n",
        "        \n",
        "        return (\n",
        "            torch.stack(states),\n",
        "            torch.stack(policies),\n",
        "            torch.tensor(values, dtype=torch.float32)\n",
        "        )\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def get_data(self):\n",
        "        return list(self.buffer)\n",
        "\n",
        "    def set_data(self, data):\n",
        "        self.buffer.extend(data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/utils/tunneling.py\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import re\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class TunnelManager:\n",
        "    \"\"\"\n",
        "    Verwaltet \u00f6ffentlichen Zugang zum Dashboard.\n",
        "    Priorit\u00e4t: ngrok \u2192 cloudflared \u2192 localtunnel\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def start(port: int = 8501) -> Tuple[Optional[str], str]:\n",
        "        # 1. Try ngrok\n",
        "        url = TunnelManager._try_ngrok(port)\n",
        "        if url:\n",
        "            return url, 'ngrok'\n",
        "        \n",
        "        # 2. Try cloudflared\n",
        "        url = TunnelManager._try_cloudflared(port)\n",
        "        if url:\n",
        "            return url, 'cloudflared'\n",
        "        \n",
        "        # 3. Try localtunnel\n",
        "        url = TunnelManager._try_localtunnel(port)\n",
        "        if url:\n",
        "            return url, 'localtunnel'\n",
        "        \n",
        "        return None, 'none'\n",
        "    \n",
        "    @staticmethod\n",
        "    def _try_ngrok(port: int) -> Optional[str]:\n",
        "        try:\n",
        "            from pyngrok import ngrok\n",
        "            \n",
        "            # Token detection\n",
        "            token = os.environ.get('NGROK_TOKEN')\n",
        "            if not token:\n",
        "                try:\n",
        "                    from google.colab import userdata\n",
        "                    token = userdata.get('NGROK_TOKEN')\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            if token:\n",
        "                ngrok.set_auth_token(token)\n",
        "            \n",
        "            tunnel = ngrok.connect(port, \"http\")\n",
        "            print(f\"\u2705 ngrok Tunnel: {tunnel.public_url}\")\n",
        "            return tunnel.public_url\n",
        "        except Exception as e:\n",
        "            # print(f\"\u26a0\ufe0f ngrok failed: {e}\")\n",
        "            return None\n",
        "    \n",
        "    @staticmethod\n",
        "    def _try_cloudflared(port: int) -> Optional[str]:\n",
        "        try:\n",
        "            # Check if cloudflared exists\n",
        "            if not os.path.exists('cloudflared'):\n",
        "                subprocess.run([\n",
        "                    'wget', '-q', \n",
        "                    'https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64',\n",
        "                    '-O', 'cloudflared'\n",
        "                ], check=True)\n",
        "                subprocess.run(['chmod', '+x', 'cloudflared'], check=True)\n",
        "            \n",
        "            proc = subprocess.Popen(\n",
        "                ['./cloudflared', 'tunnel', '--url', f'http://localhost:{port}'],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True\n",
        "            )\n",
        "            \n",
        "            # Non-blocking wait for URL\n",
        "            timeout = 15\n",
        "            start_time = time.time()\n",
        "            while time.time() - start_time < timeout:\n",
        "                line = proc.stderr.readline()\n",
        "                if 'trycloudflare.com' in line:\n",
        "                    match = re.search(r'https://[^\\s]+\\.trycloudflare\\.com', line)\n",
        "                    if match:\n",
        "                        url = match.group(0)\n",
        "                        print(f\"\u2705 cloudflared Tunnel: {url}\")\n",
        "                        return url\n",
        "                time.sleep(0.1)\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            # print(f\"\u26a0\ufe0f cloudflared failed: {e}\")\n",
        "            return None\n",
        "    \n",
        "    @staticmethod\n",
        "    def _try_localtunnel(port: int) -> Optional[str]:\n",
        "        try:\n",
        "            # Requires npx / node\n",
        "            proc = subprocess.Popen(\n",
        "                ['npx', 'localtunnel', '--port', str(port)],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True\n",
        "            )\n",
        "            \n",
        "            timeout = 10\n",
        "            start_time = time.time()\n",
        "            while time.time() - start_time < timeout:\n",
        "                line = proc.stdout.readline()\n",
        "                if 'your url is:' in line.lower():\n",
        "                    url = line.split()[-1]\n",
        "                    print(f\"\u2705 localtunnel: {url}\")\n",
        "                    return url\n",
        "                time.sleep(0.1)\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            # print(f\"\u26a0\ufe0f localtunnel failed: {e}\")\n",
        "            return None\n",
        "\n",
        "def setup_tunnel(port, ngrok_token=None):\n",
        "    if ngrok_token:\n",
        "        os.environ['NGROK_TOKEN'] = ngrok_token\n",
        "    url, method = TunnelManager.start(port)\n",
        "    return url\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/dashboard/__init__.py\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/dashboard/app.py\n",
        "import streamlit as st\n",
        "import plotly.graph_objects as go\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"PROMETHEUS-TQFD Dashboard\",\n",
        "    page_icon=\"\u265f\ufe0f\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "def find_latest_metrics_file(base_dir: Path):\n",
        "    # This is a simplified version, in reality we'd look into the run_id/metrics/ folder\n",
        "    metrics_files = list(base_dir.glob(\"**/metrics/*.jsonl\"))\n",
        "    if not metrics_files:\n",
        "        return None\n",
        "    return max(metrics_files, key=os.path.getmtime)\n",
        "\n",
        "@st.cache_data(ttl=2)\n",
        "def load_metrics(metrics_file):\n",
        "    if not metrics_file or not os.path.exists(metrics_file):\n",
        "        return []\n",
        "    \n",
        "    metrics = []\n",
        "    with open(metrics_file, 'r') as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                metrics.append(json.loads(line))\n",
        "            except:\n",
        "                pass\n",
        "    return metrics[-10000:] \n",
        "\n",
        "def get_elo(name: str, metrics: List[Dict]) -> float:\n",
        "    for m in reversed(metrics):\n",
        "        if m.get('type') == 'evaluation' and 'elo' in m:\n",
        "            return m['elo'].get(name, 1000.0)\n",
        "    return 1000.0\n",
        "\n",
        "def get_games(name: str, metrics: List[Dict]) -> int:\n",
        "    # Estimate from training steps if games not explicit\n",
        "    count = 0\n",
        "    for m in metrics:\n",
        "        if m.get('type') == f'{name}_train':\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def main():\n",
        "    st.title(\"\u265f\ufe0f PROMETHEUS-TQFD\")\n",
        "    st.markdown(\"### Dual-AI Tabula Rasa Chess Training\")\n",
        "    \n",
        "    # In a real setup, base_dir comes from environment or config\n",
        "    base_dir = Path(\"./prometheus_runs\")\n",
        "    metrics_file = find_latest_metrics_file(base_dir)\n",
        "    metrics = load_metrics(metrics_file)\n",
        "    \n",
        "    # Sidebar\n",
        "    with st.sidebar:\n",
        "        st.markdown(\"### System Status\")\n",
        "        st.metric(\"ATLAS ELO\", f\"{get_elo('atlas', metrics):.0f}\")\n",
        "        st.metric(\"ENTROPY ELO\", f\"{get_elo('entropy', metrics):.0f}\")\n",
        "        st.markdown(\"---\")\n",
        "        st.metric(\"ATLAS Progress\", f\"{get_games('atlas', metrics)} steps\")\n",
        "        st.metric(\"ENTROPY Progress\", f\"{get_games('entropy', metrics)} steps\")\n",
        "        \n",
        "        if st.button(\"Refresh\"):\n",
        "            st.rerun()\n",
        "    \n",
        "    # Tabs\n",
        "    tab1, tab2, tab3 = st.tabs([\"\ud83d\udcc8 Lernkurven\", \"\ud83d\udd25 Heatmaps\", \"\ud83c\udfae Live-Spiel\"])\n",
        "    \n",
        "    with tab1:\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.subheader(\"ATLAS Training\")\n",
        "            atlas_metrics = [m for m in metrics if m.get('type') == 'atlas_train']\n",
        "            if atlas_metrics:\n",
        "                fig = go.Figure()\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    y=[m['loss'] for m in atlas_metrics[-1000:]],\n",
        "                    name='Total Loss'\n",
        "                ))\n",
        "                fig.update_layout(yaxis_type=\"log\", title=\"Atlas Total Loss\")\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "            else:\n",
        "                st.info(\"No ATLAS metrics yet.\")\n",
        "        \n",
        "        with col2:\n",
        "            st.subheader(\"ENTROPY Training\")\n",
        "            entropy_metrics = [m for m in metrics if m.get('type') == 'entropy_train']\n",
        "            if entropy_metrics:\n",
        "                fig = go.Figure()\n",
        "                for key in ['outcome', 'mobility', 'pressure', 'stability', 'novelty']:\n",
        "                    if key in entropy_metrics[0]:\n",
        "                        fig.add_trace(go.Scatter(\n",
        "                            y=[m.get(key, 0) for m in entropy_metrics[-1000:]],\n",
        "                            name=key\n",
        "                        ))\n",
        "                fig.update_layout(yaxis_type=\"log\", title=\"Entropy Hybrid Losses\")\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "            else:\n",
        "                st.info(\"No ENTROPY metrics yet.\")\n",
        "    \n",
        "    with tab2:\n",
        "        st.info(\"Heatmaps will be implemented soon.\")\n",
        "    \n",
        "    with tab3:\n",
        "        st.info(\"Live-game view coming soon.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile prometheus_tqfd/tests.py\n",
        "import torch\n",
        "import chess\n",
        "import numpy as np\n",
        "from prometheus_tqfd.config import PrometheusConfig\n",
        "from prometheus_tqfd.encoding import BoardEncoder, MoveEncoder\n",
        "from prometheus_tqfd.atlas.network import AtlasNetwork\n",
        "from prometheus_tqfd.atlas.mcts import MCTS\n",
        "from prometheus_tqfd.entropy.network import EntropyNetworkV2\n",
        "from prometheus_tqfd.entropy.rollout import MiniRolloutSelector\n",
        "from prometheus_tqfd.tactics import TacticsDetector\n",
        "from prometheus_tqfd.physics import PhysicsFieldCalculator\n",
        "\n",
        "def test_encoding_roundtrip():\n",
        "    encoder = BoardEncoder()\n",
        "    board = chess.Board()\n",
        "    tensor = encoder.encode(board)\n",
        "    assert tensor.shape == (19, 8, 8)\n",
        "    return True\n",
        "\n",
        "def test_mcts_basic():\n",
        "    config = PrometheusConfig()\n",
        "    config.atlas_mcts_simulations = 10\n",
        "    network = AtlasNetwork(config)\n",
        "    mcts = MCTS(config, network, 'cpu')\n",
        "    board = chess.Board()\n",
        "    root = mcts.search(board)\n",
        "    assert root.visit_count > 0\n",
        "    return True\n",
        "\n",
        "def test_tactics_detector():\n",
        "    config = PrometheusConfig()\n",
        "    detector = TacticsDetector(config)\n",
        "    board = chess.Board(\"r1bqkbnr/pppp1ppp/2n5/4p3/2B1P3/5N2/PPPP1PPP/RNBQK2R b KQkq - 3 3\")\n",
        "    # Not a mate in 1 position yet, but check basic detection\n",
        "    threats = detector.detect(board)\n",
        "    assert 'mate_in_1' in threats\n",
        "    return True\n",
        "\n",
        "def test_physics_symmetry():\n",
        "    config = PrometheusConfig()\n",
        "    calc = PhysicsFieldCalculator(config)\n",
        "    board = chess.Board()\n",
        "    fields = calc.compute(board)\n",
        "    assert fields.shape == (3, 8, 8)\n",
        "    # White and Black initial positions are symmetrical\n",
        "    # Kanal 0 (Masse) sollte anfangs etwa 0-summiert sein oder symmetrisch\n",
        "    return True\n",
        "\n",
        "def run_smoke_tests():\n",
        "    print(\"\ud83e\uddea Running Smoke Tests...\")\n",
        "    tests = [\n",
        "        (\"Encoding\", test_encoding_roundtrip),\n",
        "        (\"MCTS\", test_mcts_basic),\n",
        "        (\"Tactics\", test_tactics_detector),\n",
        "        (\"Physics\", test_physics_symmetry),\n",
        "    ]\n",
        "    \n",
        "    all_passed = True\n",
        "    for name, fn in tests:\n",
        "        try:\n",
        "            if fn():\n",
        "                print(f\"  \u2705 {name} passed\")\n",
        "            else:\n",
        "                print(f\"  \u274c {name} failed\")\n",
        "                all_passed = False\n",
        "        except Exception as e:\n",
        "            print(f\"  \u274c {name} error: {e}\")\n",
        "            all_passed = False\n",
        "            \n",
        "    return all_passed\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_smoke_tests()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile main.py\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "import multiprocessing as mp\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "def setup_directories(config):\n",
        "    # Handle /content for Colab\n",
        "    if os.path.exists(\"/content\"):\n",
        "        config.base_dir = Path(\"/content/prometheus_runs\")\n",
        "    else:\n",
        "        config.base_dir = Path(\"./prometheus_runs\")\n",
        "\n",
        "    run_dir = config.base_dir / config.run_id\n",
        "    for sub in ['checkpoints', 'metrics', 'games', 'logs']:\n",
        "        (run_dir / sub).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def start_dashboard(port, run_dir):\n",
        "    print(f\"\ud83d\ude80 Starting Streamlit Dashboard on port {port}...\")\n",
        "    # Start streamlit as a subprocess\n",
        "    process = subprocess.Popen([\n",
        "        sys.executable, \"-m\", \"streamlit\", \"run\",\n",
        "        \"prometheus_tqfd/dashboard/app.py\",\n",
        "        f\"--server.port={port}\",\n",
        "        \"--server.headless=true\"\n",
        "    ])\n",
        "    return process\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\ud83d\udd25 PROMETHEUS-TQFD v2.0\")\n",
        "    print(\"   Dual-AI Tabula Rasa Chess Training System\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 1. mp setup\n",
        "    mp.set_start_method('spawn', force=True)\n",
        "\n",
        "    # 2. Hardware Detection\n",
        "    from prometheus_tqfd.utils.hardware import detect_hardware\n",
        "    hw = detect_hardware()\n",
        "    print(f\"\\n\ud83d\udcca Hardware erkannt:\")\n",
        "    print(f\"   Device: {hw.device}\")\n",
        "    if hw.gpu_name:\n",
        "        print(f\"   GPU: {hw.gpu_name} ({hw.vram_gb:.1f} GB VRAM)\")\n",
        "    print(f\"   RAM: {hw.ram_gb:.1f} GB\")\n",
        "\n",
        "    # 3. Config\n",
        "    from prometheus_tqfd.config import PrometheusConfig, adjust_config_for_hardware\n",
        "    config = PrometheusConfig()\n",
        "    config = adjust_config_for_hardware(config, hw)\n",
        "\n",
        "    # 4. Setup Directories\n",
        "    setup_directories(config)\n",
        "    run_dir = config.base_dir / config.run_id\n",
        "\n",
        "    # 5. Smoke Tests\n",
        "    from prometheus_tqfd.tests import run_smoke_tests\n",
        "    if not run_smoke_tests():\n",
        "        print(\"\u26d4 Smoke Tests failed. Training aborted.\")\n",
        "        return\n",
        "\n",
        "    # 6. Google Drive Mount (if in Colab)\n",
        "    if hw.is_colab and config.use_drive:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"\u2705 Google Drive gemountet\")\n",
        "        except:\n",
        "            print(\"\u26a0\ufe0f Google Drive nicht verf\u00fcgbar\")\n",
        "            config.use_drive = False\n",
        "\n",
        "    # 7. Start Dashboard\n",
        "    dashboard_proc = start_dashboard(config.dashboard_port, run_dir)\n",
        "    \n",
        "    # 8. Setup Tunnel\n",
        "    from prometheus_tqfd.utils.tunneling import setup_tunnel\n",
        "    ngrok_token = os.environ.get('NGROK_TOKEN')\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        ngrok_token = ngrok_token or userdata.get('NGROK_TOKEN')\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    url = setup_tunnel(config.dashboard_port, ngrok_token=ngrok_token)\n",
        "    if url:\n",
        "        print(f\"\ud83c\udf10 Dashboard URL: {url}\")\n",
        "\n",
        "    # 9. Start Supervisor\n",
        "    from prometheus_tqfd.orchestration.supervisor import Supervisor\n",
        "    supervisor = Supervisor(config)\n",
        "\n",
        "    print(\"\\n\ud83d\ude80 Starting Training Loop...\")\n",
        "    try:\n",
        "        supervisor.run()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\ud83d\uded1 Shutdown requested.\")\n",
        "    finally:\n",
        "        if dashboard_proc: dashboard_proc.terminate()\n",
        "        print(\"\u2705 Prometheus finished.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\ude80 3. Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title \u2699\ufe0f Configuration\n",
        "NGROK_TOKEN = \"\" # @param {type:\"string\"}\n",
        "USE_DRIVE = True # @param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "os.environ['NGROK_TOKEN'] = NGROK_TOKEN\n",
        "\n",
        "from main import main\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}